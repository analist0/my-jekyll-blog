---
layout: post
title: "××“×¨×™×š ××§×¦×•×¢×™: ollama - Get up and running with OpenAI gpt-oss, DeepSeek-R1, Gemma 3 and other models."
date: 2025-12-04 06:43:42 +0200
categories: [AI, LLM, ××“×¨×™×›×™×]
tags: [local-ai, llm, installation, go]
image: /assets/images/repos/ollama-20251204.png
author: AI Guide Bot
lang: he
dir: rtl
---

![ollama](/assets/images/repos/ollama-20251204.png)

# ğŸš€ ollama

**â­ 157,037 ×›×•×›×‘×™× | ğŸ”§ Go | ğŸ“… ×¢×“×›×•×Ÿ ××—×¨×•×Ÿ: 2025-12-04**

[ğŸ”— ×§×™×©×•×¨ ×œ×¨×™×¤×•](https://github.com/ollama/ollama) | [â¬‡ï¸ ×”×•×¨×“×”](https://github.com/ollama/ollama/archive/refs/heads/main.zip)

---

# ğŸ¦™ ××“×¨×™×š ×”×ª×§× ×” ××§×™×£ ×œ-Ollama - ×”×¨×¦×ª ××•×“×œ×™ AI ××§×•××™×™×

## ğŸ“‹ ×ª×•×›×Ÿ ×¢× ×™×™× ×™×
- [×¡×§×™×¨×” ×›×œ×œ×™×ª](#×¡×§×™×¨×”-×›×œ×œ×™×ª)
- [×“×¨×™×©×•×ª ××¢×¨×›×ª](#×“×¨×™×©×•×ª-××¢×¨×›×ª)
- [×”×ª×§× ×” ×¦×¢×“ ××—×¨ ×¦×¢×“](#×”×ª×§× ×”-×¦×¢×“-××—×¨-×¦×¢×“)
- [×”×’×“×¨×” ×¨××©×•× ×™×ª](#×”×’×“×¨×”-×¨××©×•× ×™×ª)
- [×©×™××•×© ×‘×¡×™×¡×™](#×©×™××•×©-×‘×¡×™×¡×™)
- [×˜×™×¤×™× ××ª×§×“××™×](#×˜×™×¤×™×-××ª×§×“××™×)
- [×¤×ª×¨×•×Ÿ ×‘×¢×™×•×ª × ×¤×•×¦×•×ª](#×¤×ª×¨×•×Ÿ-×‘×¢×™×•×ª-× ×¤×•×¦×•×ª)
- [××©××‘×™× × ×•×¡×¤×™×](#××©××‘×™×-× ×•×¡×¤×™×)

---

## ğŸ¯ ×¡×§×™×¨×” ×›×œ×œ×™×ª

### ××” ×–×” Ollama?

**Ollama** ×”×™× ×¤×œ×˜×¤×•×¨××” ××§×•××™×ª ×œ×”×¨×¦×ª ××•×“×œ×™ ×©×¤×” ×’×“×•×œ×™× (LLMs) ×¢×œ ×”××—×©×‘ ×”××™×©×™ ×©×œ×š, ×œ×œ× ×¦×•×¨×š ×‘×—×™×‘×•×¨ ×œ××™× ×˜×¨× ×˜ ××• ×©×™×¨×•×ª×™ ×¢× ×Ÿ. ×”×¤×¨×•×™×§×˜ ×–×›×” ×œ-**157,000+ ×›×•×›×‘×™×** ×‘-GitHub ×•×”×¤×š ×œ×¡×˜× ×“×¨×˜ ×“×”-×¤×§×˜×• ×œ×”×¨×¦×ª AI ××§×•××™.

### ğŸŒŸ ×œ××” Ollama?

- **ğŸ”’ ×¤×¨×˜×™×•×ª ××œ××”** - ×›×œ ×”× ×ª×•× ×™× × ×©××¨×™× ×‘××—×©×‘ ×©×œ×š
- **âš¡ ××”×™×¨×•×ª ×’×‘×•×”×”** - ×œ×œ× ×ª×œ×•×ª ×‘×¨×©×ª ××• ×‘×©×¨×ª×™× ×—×™×¦×•× ×™×™×
- **ğŸ’° ×œ×œ× ×¢×œ×•×™×•×ª** - ××™×Ÿ ×¦×•×¨×š ×‘×× ×•×™ ××• ×‘-API keys
- **ğŸ¨ ×’××™×©×•×ª ××œ××”** - ×ª××™×›×” ×‘××’×•×•×Ÿ ×¨×—×‘ ×©×œ ××•×“×œ×™× (Llama, Gemma, DeepSeek-R1 ×•×¢×•×“)
- **ğŸ› ï¸ ×§×œ ×œ×©×™××•×©** - ×××©×§ ×¤×©×•×˜ ×“××•×™ Docker

### ××§×¨×™ ×©×™××•×© ××™×“×™××œ×™×™×

âœ… ×¤×™×ª×•×— ×™×™×©×•××™ AI ××§×•××™×™×  
âœ… × ×™×ª×•×— ××¡××›×™× ×¨×’×™×©×™×  
âœ… ×¢×‘×•×“×” ×œ×œ× ×—×™×‘×•×¨ ×œ××™× ×˜×¨× ×˜  
âœ… × ×™×¡×•×™×™× ×•×œ××™×“×” ×©×œ ××•×“×œ×™ ×©×¤×”  
âœ… ×‘× ×™×™×ª chatbots ×•-assistants ××•×ª×××™× ××™×©×™×ª

---

## ğŸ’» ×“×¨×™×©×•×ª ××¢×¨×›×ª

### ×“×¨×™×©×•×ª ×—×•××¨×” ××™× ×™××œ×™×•×ª

| ×¨×›×™×‘ | ×“×¨×™×©×” ××™× ×™××œ×™×ª | ××•××œ×¥ |
|------|----------------|-------|
| **RAM** | 8GB (×œ××•×“×œ×™× ×©×œ 7B) | 16GB+ |
| | 16GB (×œ××•×“×œ×™× ×©×œ 13B) | 32GB+ |
| | 32GB (×œ××•×“×œ×™× ×©×œ 33B+) | 64GB+ |
| **××—×¡×•×Ÿ** | 10GB ×¤× ×•×™×™× | 50GB+ SSD |
| **××¢×‘×“** | CPU ××•×“×¨× ×™ (4 ×œ×™×‘×•×ª) | 8+ ×œ×™×‘×•×ª |
| **GPU** (××•×¤×¦×™×•× ×œ×™) | NVIDIA/AMD ×¢× 4GB+ VRAM | 8GB+ VRAM |

### ××¢×¨×›×•×ª ×”×¤×¢×œ×” × ×ª××›×•×ª

- âœ… **Linux** (×›×œ ×”×“×™×¡×˜×¨×™×‘×™×•×¦×™×•×ª ×”××¨×›×–×™×•×ª)
- âœ… **macOS** 11 Big Sur ×•××¢×œ×”
- âœ… **Windows** 10/11 (WSL2 ××•××œ×¥)
- âœ… **Docker** (×¢×œ ×›×œ ×¤×œ×˜×¤×•×¨××”)

### âš ï¸ ×”×¢×¨×•×ª ×—×©×•×‘×•×ª

- ××•×“×œ×™× ×’×“×•×œ×™× ×™×•×ª×¨ ×“×•×¨×©×™× ×™×•×ª×¨ ×–×™×›×¨×•×Ÿ (×¨××” ×˜×‘×œ×ª ××•×“×œ×™×)
- GPU ×××™×¥ ××©××¢×•×ª×™×ª ××ª ×”××”×™×¨×•×ª ××š ×œ× ×”×›×¨×—×™
- ××•××œ×¥ SSD ×œ×‘×™×¦×•×¢×™× ××•×¤×˜×™××œ×™×™×

---

## ğŸš€ ×”×ª×§× ×” ×¦×¢×“ ××—×¨ ×¦×¢×“

### ğŸ§ Linux

#### ×©×™×˜×” 1: ×”×ª×§× ×” ××•×˜×•××˜×™×ª (××•××œ×¥)

```bash
# ×”×•×¨×“×” ×•×”×ª×§× ×” ××•×˜×•××˜×™×ª
curl -fsSL https://ollama.com/install.sh | sh
```

**××” ×”×¡×§×¨×™×¤×˜ ×¢×•×©×”?**
- ××–×”×” ××ª ×”×“×™×¡×˜×¨×™×‘×™×•×¦×™×” ×©×œ×š
- ××•×¨×™×“ ××ª ×”×§×•×‘×¥ ×”××ª××™×
- ××’×“×™×¨ ×©×™×¨×•×ª systemd
- ××•×¡×™×£ ××ª ollama ×œ-PATH

#### ×©×™×˜×” 2: ×”×ª×§× ×” ×™×“× ×™×ª (Ubuntu/Debian)

```bash
# 1. ×”×•×¨×“×ª ×”×‘×™× ××¨×™
curl -L https://ollama.com/download/ollama-linux-amd64 -o ollama

# 2. ×”×¤×™×›×ª×• ×œ×§×•×‘×¥ ×”×¤×¢×œ×”
chmod +x ollama

# 3. ×”×¢×‘×¨×” ×œ×ª×™×§×™×™×ª ××¢×¨×›×ª
sudo mv ollama /usr/local/bin/

# 4. ×™×¦×™×¨×ª ××©×ª××© ××¢×¨×›×ª
sudo useradd -r -s /bin/false -m -d /usr/share/ollama ollama

# 5. ×™×¦×™×¨×ª ×§×•×‘×¥ ×©×™×¨×•×ª systemd
sudo tee /etc/systemd/system/ollama.service > /dev/null <<EOF
[Unit]
Description=Ollama Service
After=network-online.target

[Service]
ExecStart=/usr/local/bin/ollama serve
User=ollama
Group=ollama
Restart=always
RestartSec=3

[Install]
WantedBy=default.target
EOF

# 6. ×”×¤×¢×œ×ª ×”×©×™×¨×•×ª
sudo systemctl daemon-reload
sudo systemctl enable ollama
sudo systemctl start ollama
```

#### ××™××•×ª ×”×”×ª×§× ×”

```bash
# ×‘×“×™×§×ª ×’×¨×¡×”
ollama --version

# ×‘×“×™×§×ª ×¡×˜×˜×•×¡ ×”×©×™×¨×•×ª
systemctl status ollama

# ×‘×“×™×§×ª ×—×™×‘×•×¨ ×œ×©×¨×ª
curl http://localhost:11434
```

---

### ğŸ macOS

#### ×”×ª×§× ×”

```bash
# 1. ×”×•×¨×“×ª ×§×•×‘×¥ ×”×”×ª×§× ×”
curl -L https://ollama.com/download/Ollama.dmg -o Ollama.dmg

# 2. ×¤×ª×™×—×ª ×§×•×‘×¥ ×”-DMG
open Ollama.dmg
```

**××• ×‘×××¦×¢×•×ª Homebrew:**

```bash
# ×”×ª×§× ×” ×“×¨×š Homebrew
brew install ollama

# ×”×¤×¢×œ×ª ×”×©×™×¨×•×ª
brew services start ollama
```

#### ××™××•×ª

```bash
# ×‘×“×™×§×” ×©-Ollama ×¤×•×¢×œ
ollama --version

# ×”×¨×¦×ª ××•×“×œ ×¨××©×•×Ÿ
ollama run gemma3:1b
```

---

### ğŸªŸ Windows

#### ×©×™×˜×” 1: ××ª×§×™×Ÿ ×¨×©××™ (××•××œ×¥ ×œ××ª×—×™×œ×™×)

1. **×”×•×¨×“×”:**
   ```
   https://ollama.com/download/OllamaSetup.exe
   ```

2. **×”×ª×§× ×”:**
   - ×”×¤×¢×œ ××ª ×”×§×•×‘×¥ ×©×”×•×¨×“×ª
   - ×¢×§×•×‘ ××—×¨ ××©×£ ×”×”×ª×§× ×”
   - Ollama ×™×ª×•×•×¡×£ ××•×˜×•××˜×™×ª ×œ-PATH

3. **××™××•×ª:**
   ```powershell
   # ×¤×ª×— PowerShell ××• CMD
   ollama --version
   ```

#### ×©×™×˜×” 2: WSL2 (××•××œ×¥ ×œ××ª×§×“××™×)

```bash
# 1. ×”×¤×¢×œ WSL2 ×¢× Ubuntu
wsl --install

# 2. ×‘×ª×•×š WSL, ×”×ª×§×Ÿ Ollama
curl -fsSL https://ollama.com/install.sh | sh

# 3. ×”×¤×¢×œ ××ª ×”×©×™×¨×•×ª
ollama serve
```

**×™×ª×¨×•× ×•×ª WSL2:**
- ×‘×™×¦×•×¢×™× ×˜×•×‘×™× ×™×•×ª×¨ ×œ××•×“×œ×™× ×’×“×•×œ×™×
- ×ª××™××•×ª ××œ××” ×œ××¢×¨×›×ª Linux
- ×ª××™×›×” ×˜×•×‘×” ×™×•×ª×¨ ×‘-GPU (×¢× CUDA)

---

### ğŸ³ Docker

#### ×”×ª×§× ×” ×‘×¡×™×¡×™×ª

```bash
# ×”×¨×¦×ª Ollama ×‘×§×•× ×˜×™×™× ×¨
docker run -d -v ollama:/root/.ollama -p 11434:11434 --name ollama ollama/ollama
```

#### ×¢× ×ª××™×›×ª GPU (NVIDIA)

```bash
# ×”×ª×§×Ÿ NVIDIA Container Toolkit ×ª×—×™×œ×”
# https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/install-guide.html

# ×”×¨×¦×” ×¢× GPU
docker run -d --gpus=all -v ollama:/root/.ollama -p 11434:11434 --name ollama ollama/ollama
```

#### Docker Compose (××•××œ×¥ ×œ×¤×¨×•×™×§×˜×™×)

```yaml
# docker-compose.yml
version: '3.8'

services:
  ollama:
    image: ollama/ollama:latest
    container_name: ollama
    ports:
      - "11434:11434"
    volumes:
      - ollama_data:/root/.ollama
    restart: unless-stopped
    # ×”×•×¡×¤×ª GPU (××•×¤×¦×™×•× ×œ×™)
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]

volumes:
  ollama_data:
```

```bash
# ×”×¤×¢×œ×”
docker-compose up -d

# ×¦×¤×™×™×” ×‘×œ×•×’×™×
docker-compose logs -f
```

---

### ğŸ“± Termux (Android)

> âš ï¸ **×©×™× ×œ×‘:** ×‘×™×¦×•×¢×™× ××•×’×‘×œ×™×, ××ª××™× ×¨×§ ×œ××•×“×œ×™× ×§×˜× ×™× (1B-3B)

```bash
# 1. ×”×ª×§×Ÿ Termux ×-F-Droid (×œ× ×-Play Store)

# 2. ×¢×“×›×•×Ÿ ×—×‘×™×œ×•×ª
pkg update && pkg upgrade

# 3. ×”×ª×§× ×ª ×ª×œ×•×™×•×ª
pkg install wget proot-distro

# 4. ×”×ª×§× ×ª Ubuntu ×‘×ª×•×š Termux
proot-distro install ubuntu
proot-distro login ubuntu

# 5. ×‘×ª×•×š Ubuntu, ×”×ª×§×Ÿ Ollama
curl -fsSL https://ollama.com/install.sh | sh

# 6. ×”×¨×¦×”
ollama serve &
ollama run gemma3:1b
```

---

## âš™ï¸ ×”×’×“×¨×” ×¨××©×•× ×™×ª

### ×”×•×¨×“×ª ×”××•×“×œ ×”×¨××©×•×Ÿ

```bash
# ××•×“×œ ×§×œ ×•××”×™×¨ ×œ×œ××™×“×” (815MB)
ollama pull gemma3:1b

# ××•×“×œ ×××•×–×Ÿ (3.3GB)
ollama pull gemma3

# ××•×“×œ ×—×–×§ ×œ×¢×‘×¨×™×ª (4.7GB)
ollama pull llama3.1
```

### ğŸ”§ ×”×’×“×¨×•×ª ×¡×‘×™×‘×”

#### ×©×™× ×•×™ ×ª×™×§×™×™×ª ××—×¡×•×Ÿ ×”××•×“×œ×™×

```bash
# Linux/Mac - ×”×•×¡×£ ×œ-~/.bashrc ××• ~/.zshrc
export OLLAMA_MODELS="/path/to/your/models"

# Windows - ×”×’×“×¨ ××©×ª× ×” ×¡×‘×™×‘×”
setx OLLAMA_MODELS "C:\path\to\models"
```

#### ×©×™× ×•×™ ×¤×•×¨×˜ ×”×©×¨×ª

```bash
# ×”×’×“×¨×ª ×¤×•×¨×˜ ××—×¨ (×‘×¨×™×¨×ª ××—×“×œ: 11434)
export OLLAMA_HOST="0.0.0.0:8080"

# ×”×¤×¢×œ×” ××—×“×© ×©×œ ×”×©×™×¨×•×ª
sudo systemctl restart ollama  # Linux
brew services restart ollama   # macOS
```

#### ×”×’×‘×œ×ª ×©×™××•×© ×‘×–×™×›×¨×•×Ÿ

```bash
# ×”×’×‘×œ×ª ×©×™××•×© ×‘-RAM (×‘×’×™×’×”-×‘×™×™×˜×™×)
export OLLAMA_MAX_LOADED_MODELS=1
export OLLAMA_NUM_PARALLEL=1
```

### ğŸ¨ ×”×ª×××ª ××•×“×œ ××™×©×™ (Modelfile)

×¦×•×¨ ×§×•×‘×¥ `Modelfile`:

```dockerfile
# Modelfile - ×“×•×’××” ×œ××•×“×œ ××•×ª××
FROM llama3.1

# ×”×’×“×¨×ª ×¤×¨××˜×¨×™×
PARAMETER temperature 0.7
PARAMETER top_p 0.9
PARAMETER top_k 40

# ×”×’×“×¨×ª ×”×§×©×¨ ××¢×¨×›×ª ×‘×¢×‘×¨×™×ª
SYSTEM """
××ª×” ×¢×•×–×¨ AI ×™×“×™×“×•×ª×™ ×•××•×¢×™×œ ×©××“×‘×¨ ×¢×‘×¨×™×ª ×‘×¦×•×¨×” ×˜×‘×¢×™×ª.
××ª×” ×¢×•× ×” ×‘×ª××¦×™×ª×™×•×ª ×•×‘×‘×”×™×¨×•×ª, ×•××¡×¤×§ ×“×•×’×××•×ª ×›×©×¦×¨×™×š.
"""
```

×™×¦×™×¨×ª ×”××•×“×œ ×”××•×ª××:

```bash
# ×™×¦×™×¨×ª ××•×“×œ ×—×“×©
ollama create my-hebrew-assistant -f Modelfile

# ×”×¨×¦×”
ollama run my-hebrew-assistant
```

---

## ğŸ“– ×©×™××•×© ×‘×¡×™×¡×™

### ×××©×§ ×©×•×¨×ª ×¤×§×•×“×” (CLI)

#### ×©×™×—×” ××™× ×˜×¨××§×˜×™×‘×™×ª

```bash
# ×”×¨×¦×ª ××•×“×œ ×‘××¦×‘ ×©×™×—×”
ollama run gemma3

# ×“×•×’××ª ×©×™×—×”:
>>> ××” ×–×” ×‘×™× ×” ××œ××›×•×ª×™×ª?
×‘×™× ×” ××œ××›×•×ª×™×ª ×”×™× ×ª×—×•× ×‘××“×¢×™ ×”××—×©×‘...

>>> /bye  # ×™×¦×™××” ××”×©×™×—×”
```

#### ×¤×§×•×“×•×ª ×©×™××•×©×™×•×ª ×‘×©×™×—×”

```
/help           - ×”×¦×’×ª ×¢×–×¨×”
/bye ××• /exit  - ×™×¦×™××”
/clear          - × ×™×§×•×™ ××¡×š
/save <×©×>      - ×©××™×¨×ª ×©×™×—×”
/load <×©×>      - ×˜×¢×™× ×ª ×©×™×—×”
```

#### ×©××™×œ×ª×” ×—×“-×¤×¢××™×ª

```bash
# ×©×œ×™×—×ª ×”×•×“×¢×” ×‘×•×“×“×ª
echo "×›×ª×•×‘ ×©×™×¨ ×§×¦×¨ ×¢×œ ×”×™×" | ollama run gemma3

# ×¢× redirect ×©×œ ×¤×œ×˜
ollama run gemma3 "×¡×›× ××ª ×”×˜×§×¡×˜ ×”×‘×: $(cat document.txt)" > summary.txt
```

### ğŸ”§ ×¤×§×•×“×•×ª × ×™×”×•×œ

```bash
# ×¨×©×™××ª ×›×œ ×”××•×“×œ×™× ×”××•×ª×§× ×™×
ollama list

# ××—×™×§×ª ××•×“×œ
ollama rm llama3.1

# ×”×¦×’×ª ××™×“×¢ ×¢×œ ××•×“×œ
ollama show gemma3

# ×¢×¦×™×¨×ª ×©×¨×ª
pkill ollama  # Linux/Mac
taskkill /F /IM ollama.exe  # Windows
```

---

## ğŸ ×©×™××•×© ×‘×¡×¤×¨×™×•×ª ×ª×›× ×•×ª

### Python

#### ×”×ª×§× ×”

```bash
pip install ollama
```

#### ×“×•×’×××•×ª ×§×•×“

**×“×•×’××” ×‘×¡×™×¡×™×ª:**

```python
import ollama

# ×©×œ×™×—×ª ×©××™×œ×ª×” ×¤×©×•×˜×”
response = ollama.chat(
    model='gemma3',
    messages=[
        {
            'role': 'user',
            'content': '××” ×–×” Python?'
        }
    ]
)

print(response['message']['content'])
```

**×©×™×—×” ×¢× ×”×§×©×¨:**

```python
import ollama

messages = []

def chat(user_input):
    # ×”×•×¡×¤×ª ×”×•×“×¢×ª ××©×ª××©
    messages.append({
        'role': 'user',
        'content': user_input
    })
    
    # ×§×‘×œ×ª ×ª×©×•×‘×”
    response = ollama.chat(
        model='gemma3',
        messages=messages
    )
    
    # ×”×•×¡×¤×ª ×ª×©×•×‘×ª ×”××•×“×œ ×œ×”×™×¡×˜×•×¨×™×”
    messages.append({
        'role': 'assistant',
        'content': response['message']['content']
    })
    
    return response['message']['content']

# ×©×™××•×©
print(chat("×”×™×™, ××” ×©×œ×•××š?"))
print(chat("×ª×•×›×œ ×œ×¢×–×•×¨ ×œ×™ ×¢× Python?"))
```

**Streaming (×ª×©×•×‘×” ×‘×–××Ÿ ×××ª):**

```python
import ollama

stream = ollama.chat(
    model='gemma3',
    messages=[{'role': 'user', 'content': '×›×ª×•×‘ ×¡×™×¤×•×¨ ×§×¦×¨'}],
    stream=True
)

for chunk in stream:
    print(chunk['message']['content'], end='', flush=True)
```

**×™×¦×™×¨×ª embeddings:**

```python
import ollama

# ×”××¨×ª ×˜×§×¡×˜ ×œ×•×•×§×˜×•×¨
embeddings = ollama.embeddings(
    model='nomic-embed-text',
    prompt='×–×”×• ×˜×§×¡×˜ ×œ×“×•×’××”'
)

print(embeddings['embedding'])  # ×¨×©×™××” ×©×œ ××¡×¤×¨×™×
```

---

### JavaScript/Node.js

#### ×”×ª×§× ×”

```bash
npm install ollama
```

#### ×“×•×’×××•×ª ×§×•×“

**×“×•×’××” ×‘×¡×™×¡×™×ª:**

```javascript
import ollama from 'ollama';

async function main() {
  const response = await ollama.chat({
    model: 'gemma3',
    messages: [
      { role: 'user', content: '××” ×–×” JavaScript?' }
    ]
  });
  
  console.log(response.message.content);
}

main();
```

**Streaming:**

```javascript
import ollama from 'ollama';

async function streamChat() {
  const stream = await ollama.chat({
    model: 'gemma3',
    messages: [{ role: 'user', content: '×¡×¤×¨ ×œ×™ ×‘×“×™×—×”' }],
    stream: true
  });

  for await (const chunk of stream) {
    process.stdout.write(chunk.message.content);
  }
}

streamChat();
```

---

### ğŸŒ REST API

Ollama ××¡×¤×§ REST API ××œ×:

#### ×’× ×¨×¦×™×” ×‘×¡×™×¡×™×ª

```bash
curl http://localhost:11434/api/generate -d '{
  "model": "gemma3",
  "prompt": "××“×•×¢ ×”×©××™×™× ×›×—×•×œ×™×?",
  "stream": false
}'
```

#### ×©×™×—×” ×¢× ×”×§×©×¨

```bash
curl http://localhost:11434/api/chat -d '{
  "model": "gemma3",
  "messages": [
    {
      "role": "system",
      "content": "××ª×” ××•××—×” ×œ×¤×™×–×™×§×”"
    },
    {
      "role": "user",
      "content": "×”×¡×‘×¨ ×¢×œ ××•×¨"
    }
  ],
  "stream": false
}'
```

#### Python ×¢× requests

```python
import requests
import json

def ollama_generate(prompt, model="gemma3"):
    url = "http://localhost:11434/api/generate"
    payload = {
        "model": model,
        "prompt": prompt,
        "stream": False
    }
    
    response = requests.post(url, json=payload)
    return response.json()['response']

# ×©×™××•×©
result = ollama_generate("××” ×–×” ×œ××™×“×ª ××›×•× ×”?")
print(result)
```

---

## ğŸš€ ×˜×™×¤×™× ××ª×§×“××™×

### âš¡ ××•×¤×˜×™××™×–×¦×™×” ×œ×‘×™×¦×•×¢×™×

#### 1. ×©×™××•×© ×‘-GPU

```bash
# ×•×“× ×©×™×© ×œ×š CUDA ××•×ª×§×Ÿ (NVIDIA)
nvidia-smi

# Ollama ×™×–×”×” ××•×˜×•××˜×™×ª ××ª ×”-GPU
# ×œ××™××•×ª, ×‘×“×•×§ ×‘×œ×•×’×™×:
journalctl -u ollama -f

# ×¦×¤×•×™ ×œ×¨××•×ª: "Using NVIDIA GPU"
```

#### 2. ×‘×—×™×¨×ª quantization × ×›×•×Ÿ

××•×“×œ×™× ××’×™×¢×™× ×‘×’×¨×¡××•×ª quantization ×©×•× ×•×ª:

| Quantization | ×’×•×“×œ | ××™×›×•×ª | ××”×™×¨×•×ª | ××ª×™ ×œ×”×©×ª××© |
|--------------|------|-------|--------|------------|
| **Q4_0** | ×§×˜×Ÿ ×××•×“ | ×¡×‘×™×¨×” | ××”×™×¨×” ×××•×“ | RAM ××•×’×‘×œ |
| **Q5_K_M** | ×‘×™× ×•× ×™ | ×˜×•×‘×” | ××”×™×¨×” | ×××•×–×Ÿ (××•××œ×¥) |
| **Q6_K** | ×’×“×•×œ ×™×•×ª×¨ | ××¦×•×™× ×ª | ×‘×™× ×•× ×™×ª | ×™×© ×”×¨×‘×” RAM |
| **F16** | ×”×›×™ ×’×“×•×œ | ××•×©×œ××ª | ××™×˜×™×ª | GPU ×—×–×§ |

```bash
# ×“×•×’××”: ×”×•×¨×“×ª ××•×“×œ ×‘×’×¨×¡×” ×××•×˜×‘×ª
ollama pull llama3.1:8b-q5_k_m
```

#### 3. × ×™×”×•×œ ×–×™×›×¨×•×Ÿ ×™×¢×™×œ

```bash
# ×”×’×‘×œ ××¡×¤×¨ ××•×“×œ×™× ×‘×–×™×›×¨×•×Ÿ ×‘×•-×–×× ×™×ª
export OLLAMA_MAX_LOADED_MODELS=1

# ×”×’×“×¨ timeout ×œ×¤×¨×™×§×ª ××•×“×œ×™× (×‘×“×§×•×ª)
export OLLAMA_KEEP_ALIVE=5m

# ×”×©×‘×ª keep-alive ×œ×—×œ×•×˜×™×Ÿ (×¤×¨×™×§×” ××™×™×“×™×ª)
export OLLAMA_KEEP_ALIVE=0
```

#### 4. ×©×™××•×© ×‘××—×©×‘×™× ××¨×•×—×§×™×

```bash
# ×‘××—×©×‘ ×”×©×¨×ª:
OLLAMA_HOST=0.0.0.0:11434 ollama serve

# ×‘××—×©×‘ ×”×œ×§×•×—:
export OLLAMA_HOST=http://192.168.1.100:11434
ollama list  # ×¨×©×™××ª ××•×“×œ×™× ××”×©×¨×ª ×”××¨×•×—×§
```

---

### ğŸ¨ ×™×¦×™×¨×ª Modelfiles ××ª×§×“××™×

#### ××•×“×œ ×¢× ×¤×¨×¡×•× ×”

```dockerfile
FROM llama3.1

# ×¤×¨××˜×¨×™× ×œ×™×¦×™×¨×ª×™×•×ª
PARAMETER temperature 1.2
PARAMETER top_p 0.95
PARAMETER repeat_penalty 1.1

SYSTEM """
××ª×” ×¡×•×¤×¨ ××¤×•×¨×¡× ×‘×©× ×™×¨×•×Ÿ ×©××ª××—×” ×‘×¡×™×¤×•×¨×™× ××•×¤×¨×›×™×.
××ª×” ×›×•×ª×‘ ×‘×¡×’× ×•×Ÿ ×”×•××•×¨×™×¡×˜×™ ×•×“××™×•× ×™.
×ª××™×“ ××ª×—×™×œ ×¡×™×¤×•×¨×™× ×‘"×¤×¢× ××—×ª..."
"""

TEMPLATE """
{{ .System }}

×¡×•×¤×¨: {{ .Prompt }}
"""
```

#### ××•×“×œ ×œ×§×•×“ ×ª×›× ×•×ª

```dockerfile
FROM codellama

PARAMETER temperature 0.2
PARAMETER top_k 10

SYSTEM """
××ª×” ××ª×›× ×ª ××•××—×”. ××ª×” ×¢×•× ×” ×¨×§ ×‘×§×•×“ ×ª×§×™×Ÿ ×•×××•×˜×‘.
×œ×¤× ×™ ×›×œ ×§×•×“, ×ª×›×ª×•×‘ ×”×¡×‘×¨ ×§×¦×¨ ×‘×©×•×¨×” ××—×ª.
"""
```

×©××™×¨×” ×•×”×¨×¦×”:

```bash
ollama create code-expert -f Modelfile
ollama run code-expert "×›×ª×•×‘ ×¤×•× ×§×¦×™×” ×œ××™×•×Ÿ ×‘×•×¢×•×ª ×‘-Python"
```

---

### ğŸ” ×”×ª×××” ×œ××¡××›×™× ××§×•××™×™× (RAG)

**RAG** = Retrieval-Augmented Generation

```python
import ollama
import os

def read_documents(folder_path):
    """×§×¨×™××ª ×›×œ ×”××¡××›×™× ××ª×™×§×™×™×”"""
    documents = []
    for filename in os.listdir(folder_path):
        if filename.endswith('.txt'):
            with open(os.path.join(folder_path, filename), 'r', encoding='utf-8') as f:
                documents.append(f.read())
    return documents

def create_context(documents, query):
    """×™×¦×™×¨×ª ×”×§×©×¨ ×¨×œ×•×•× ×˜×™"""
    # ×›××Ÿ ××¤×©×¨ ×œ×”×•×¡×™×£ ×—×™×¤×•×© ××ª×§×“× ×™×•×ª×¨
    context = "\n\n".join(documents[:3])  # 3 ××¡××›×™× ×¨××©×•× ×™×
    return context

def rag_query(query, documents_folder):
    """×©××™×œ×ª×” ×¢× RAG"""
    docs = read_documents(documents_folder)
    context = create_context(docs, query)
    
    prompt = f"""
    ×‘×”×ª×‘×¡×¡ ×¢×œ ×”××¡××›×™× ×”×‘××™×:
    
    {context}
    
    ×©××œ×”: {query}
    
    ×ª×©×•×‘×”:
    """
    
    response = ollama.generate(
        model='gemma3',
        prompt=prompt
    )
    
    return response['response']

# ×©×™××•×©
answer = rag_query(
    "××” ×”××“×™× ×™×•×ª ×œ×’×‘×™ ×—×•×¤×©×•×ª?",
    "./company_docs"
)
print(answer)
```

---

### ğŸ“Š × ×™×˜×•×¨ ×‘×™×¦×•×¢×™×

```python
import ollama
import time

def benchmark_model(model, prompt, runs=5):
    """×‘×“×™×§×ª ×‘×™×¦×•×¢×™ ××•×“×œ"""
    times = []
    
    for i in range(runs):
        start = time.time()
        ollama.generate(model=model, prompt=prompt)
        elapsed = time.time() - start
        times.append(elapsed)
        print(f"×¨×™×¦×” {i+1}: {elapsed:.2f} ×©× ×™×•×ª")
    
    avg_time = sum(times) / len(times)
    print(f"\n×××•×¦×¢: {avg_time:.2f} ×©× ×™×•×ª")
    print(f"××”×™×¨ ×‘×™×•×ª×¨: {min(times):.2f} ×©× ×™×•×ª")
    print(f"××™×˜×™ ×‘×™×•×ª×¨: {max(times):.2f} ×©× ×™×•×ª")

# ×©×™××•×©
benchmark_model(
    model='gemma3:1b',
    prompt='×›×ª×•×‘ ××©×¤×˜ ×§×¦×¨ ×¢×œ ××–×’ ×”××•×•×™×¨'
)
```

---

## ğŸ› ï¸ ×¤×ª×¨×•×Ÿ ×‘×¢×™×•×ª × ×¤×•×¦×•×ª

### âŒ ×©×’×™××”: "connection refused"

**×¡×™×‘×”:** ×”×©×¨×ª ×œ× ×¨×¥

**×¤×ª×¨×•×Ÿ:**

```bash
# Linux
sudo systemctl start ollama
sudo systemctl status ollama

# macOS
brew services start ollama

# Windows - ×”×¨×¥ ××—×“×© ××ª ×”××¤×œ×™×§×¦×™×” ××•:
ollama serve
```

---

### âŒ ×©×’×™××”: "out of memory"

**×¡×™×‘×”:** ×”××•×“×œ ×’×“×•×œ ××“×™ ×œ-RAM

**×¤×ª×¨×•× ×•×ª:**

```bash
# 1. ×”×©×ª××© ×‘××•×“×œ ×§×˜×Ÿ ×™×•×ª×¨
ollama pull gemma3:1b  # ×‘××§×•× gemma3:27b

# 2. ×”×©×ª××© ×‘-quantization × ××•×š ×™×•×ª×¨
ollama pull llama3.1:8b-q4_0  # ×‘××§×•× q6_k

# 3. ×¡×’×•×¨ ×™×™×©×•××™× ××—×¨×™×

# 4. ×”×’×“×œ swap (Linux)
sudo fallocate -l 8G /swapfile
sudo chmod 600 /swapfile
sudo mkswap /swapfile
sudo swapon /swapfile
```

---

### âŒ ×©×’×™××”: "model not found"

**×¤×ª×¨×•×Ÿ:**

```bash
# ×•×“× ×©×”××•×“×œ ××•×ª×§×Ÿ
ollama list

# ×× ×œ×, ×”×•×¨×“ ××•×ª×•
ollama pull gemma3

# ×‘×“×•×§ ××™×•×ª ×©× ×”××•×“×œ (case-sensitive)
ollama run gemma3  # ×•×œ× Gemma3
```

---

### âŒ ×ª×’×•×‘×•×ª ××™×˜×™×•×ª ×××•×“

**××‘×—×•×Ÿ:**

```bash
# ×‘×“×•×§ ×©×™××•×© ×‘-CPU
htop  # Linux/Mac
# Task Manager  # Windows

# ×‘×“×•×§ ×× GPU ××–×•×”×”
nvidia-smi  # NVIDIA
rocm-smi    # AMD

# ×‘×“×•×§ ×œ×•×’×™×
journalctl -u ollama -n 50  # Linux
```

**×¤×ª×¨×•× ×•×ª:**

1. **×•×“× ×©×™××•×© ×‘-GPU:**
   ```bash
   # ×”×ª×§×Ÿ CUDA drivers
   # ×¨××”: https://developer.nvidia.com/cuda-downloads
   
   # ××™××•×ª
   nvidia-smi
   ```

2. **×”×©×ª××© ×‘××•×“×œ ×§×œ ×™×•×ª×¨:**
   ```bash
   ollama pull phi4-mini  # 3.8B - ××”×™×¨ ×××•×“
   ```

3. **×”×¤×—×ª ×”×’×“×¨×•×ª ××™×›×•×ª:**
   ```python
   ollama.generate(
       model='gemma3',
       prompt='...',
       options={
           'num_ctx': 2048,  # ×”×§×˜×Ÿ ×”×§×©×¨ (×‘×¨×™×¨×ª ××—×“×œ: 4096)
           'num_predict': 100  # ×”×’×‘×œ ××•×¨×š ×ª×©×•×‘×”
       }
   )
   ```

---

### âŒ ×‘×¢×™×•×ª ×¢× ×¢×‘×¨×™×ª

**×ª×¡××™× ×™×:** ×˜×§×¡×˜ ××©×•×‘×©, ×ª×•×•×™× ×œ× ×§×¨×™××™×

**×¤×ª×¨×•×Ÿ:**

```python
# ×•×“× encoding × ×›×•×Ÿ ×‘-Python
import sys
sys.stdout.reconfigure(encoding='utf-8')

# ××• ×‘×©×•×¨×ª ×¤×§×•×“×”:
export PYTHONIOENCODING=utf-8  # Linux/Mac
chcp 65001  # Windows CMD
```

**×‘×—×™×¨×ª ××•×“×œ ×˜×•×‘ ×œ×¢×‘×¨×™×ª:**

```bash
# ××•××œ×¥:
ollama pull llama3.1       # ×ª××™×›×” ××¦×•×™× ×ª
ollama pull gemma3         # ×ª××™×›×” ×˜×•×‘×”
ollama pull qwq            # ×ª××™×›×” ×˜×•×‘×”

# ×¤×—×•×ª ××•××œ×¥:
ollama pull mistral  # ×ª××™×›×” ×—×œ×§×™×ª ×‘×¢×‘×¨×™×ª
```

---

### âŒ ×©×’×™××”: "invalid model file"

**×¤×ª×¨×•×Ÿ:**

```bash
# ××—×§ ×

---

## ğŸ“Š ×¡×˜×˜×™×¡×˜×™×§×•×ª ×”×¤×¨×•×™×§×˜

- **×›×•×›×‘×™×**: 157,037 â­
- **Forks**: 13,822 ğŸ”±
- **Issues**: 2,316 ğŸ›
- **×©×¤×”**: Go ğŸ’»
- **×¨×™×©×™×•×Ÿ**: MIT License ğŸ“œ

## ğŸ”— ×§×™×©×•×¨×™× ×©×™××•×©×™×™×

- [×¨×™×¤×• ×‘-GitHub](https://github.com/ollama/ollama)
- [Issues & ×ª××™×›×”](https://github.com/ollama/ollama/issues)
- [Discussions](https://github.com/ollama/ollama/discussions)
- [Wiki](https://github.com/ollama/ollama/wiki)

---

*××“×¨×™×š ×–×” × ×•×¦×¨ ××•×˜×•××˜×™×ª ×¢×œ ×™×“×™ AI Guide Bot ×¢× Claude AI*
*×¢×“×›×•×Ÿ ××—×¨×•×Ÿ: 04/12/2025 06:43*
