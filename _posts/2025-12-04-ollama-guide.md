---
layout: unified-post
title: "××“×¨×™×š ××§×¦×•×¢×™: ollama - Get up and running with OpenAI gpt-oss, DeepSeek-R1, Gemma 3 and other models."
date: 2025-12-04 12:26:40 +0200
categories: [AI, LLM, ××“×¨×™×›×™×]
tags: [local-ai, llm, installation, go]
image: /assets/images/repos/ollama-20251204.png
author: AI Guide Bot
lang: he
dir: rtl
---

![ollama](/assets/images/repos/ollama-20251204.png)

# ğŸš€ ollama

**â­ 157,060 ×›×•×›×‘×™× | ğŸ”§ Go | ğŸ“… ×¢×“×›×•×Ÿ ××—×¨×•×Ÿ: 2025-12-04**

[ğŸ”— ×§×™×©×•×¨ ×œ×¨×™×¤×•](https://github.com/ollama/ollama) | [â¬‡ï¸ ×”×•×¨×“×”](https://github.com/ollama/ollama/archive/refs/heads/main.zip)

---

# ğŸ“˜ ××“×¨×™×š ×”×ª×§× ×” ××§×™×£ ×œ-Ollama - ×”×¨×¦×ª ××•×“×œ×™ AI ××§×•××™×ª

<div align="center">

![Ollama](https://img.shields.io/github/stars/ollama/ollama?style=social)
![License](https://img.shields.io/badge/license-MIT-blue)
![Go](https://img.shields.io/badge/Go-00ADD8?logo=go&logoColor=white)

</div>

---

## ğŸ“‹ ×ª×•×›×Ÿ ×¢× ×™×™× ×™×

1. [×¡×§×™×¨×” ×›×œ×œ×™×ª](#-×¡×§×™×¨×”-×›×œ×œ×™×ª)
2. [×“×¨×™×©×•×ª ××¢×¨×›×ª](#-×“×¨×™×©×•×ª-××¢×¨×›×ª)
3. [×”×ª×§× ×” ×¦×¢×“ ××—×¨ ×¦×¢×“](#-×”×ª×§× ×”-×¦×¢×“-××—×¨-×¦×¢×“)
4. [×”×’×“×¨×” ×¨××©×•× ×™×ª](#-×”×’×“×¨×”-×¨××©×•× ×™×ª)
5. [×©×™××•×© ×‘×¡×™×¡×™](#-×©×™××•×©-×‘×¡×™×¡×™)
6. [×˜×™×¤×™× ××ª×§×“××™×](#-×˜×™×¤×™×-××ª×§×“××™×)
7. [×¤×ª×¨×•×Ÿ ×‘×¢×™×•×ª × ×¤×•×¦×•×ª](#-×¤×ª×¨×•×Ÿ-×‘×¢×™×•×ª-× ×¤×•×¦×•×ª)
8. [××©××‘×™× × ×•×¡×¤×™×](#-××©××‘×™×-× ×•×¡×¤×™×)

---

## ğŸ¯ ×¡×§×™×¨×” ×›×œ×œ×™×ª

### ××” ×–×” Ollama?

**Ollama** ×”×™× ×¤×œ×˜×¤×•×¨××” ×§×œ×ª ××©×§×œ ×•×¤×©×•×˜×” ×œ×”×¨×¦×” ×©×œ ××•×“×œ×™ ×©×¤×” ×’×“×•×œ×™× (LLM) ×‘××•×¤×Ÿ **××§×•××™** ×¢×œ ×”××—×©×‘ ×©×œ×š. ×‘××§×•× ×œ×”×¡×ª××š ×¢×œ ×©×™×¨×•×ª×™ ×¢× ×Ÿ ×™×§×¨×™× ××• ×œ×©×ª×£ ××™×“×¢ ×¨×’×™×© ×¢× ×¦×“×“×™× ×©×œ×™×©×™×™×, Ollama ×××¤×©×¨×ª ×œ×š:

âœ¨ **×™×ª×¨×•× ×•×ª ××¨×›×–×™×™×:**
- ğŸ”’ **×¤×¨×˜×™×•×ª ××œ××”** - ×”× ×ª×•× ×™× ×©×œ×š × ×©××¨×™× ×‘××—×©×‘ ×©×œ×š
- ğŸ’° **×—×™× ××™ ×œ×—×œ×•×˜×™×Ÿ** - ××™×Ÿ ×¢×œ×•×™×•×ª API ××• ×× ×•×™×™×
- âš¡ **××”×™×¨ ×•×™×¢×™×œ** - ××•×¤×˜×™××™×–×¦×™×” ××ª×§×“××ª ×œ×—×•××¨×” ××§×•××™×ª
- ğŸ¨ **×’××™×©** - ×ª××™×›×” ×‘××’×•×•×Ÿ ×¨×—×‘ ×©×œ ××•×“×œ×™× ×¤×ª×•×—×™×
- ğŸ”Œ **××•×¤×œ×™×™×Ÿ** - ×¢×‘×•×“×” ×œ×œ× ×—×™×‘×•×¨ ×œ××™× ×˜×¨× ×˜

### ×œ××™ ×–×” ××ª××™×?

- ğŸ‘¨â€ğŸ’» ××¤×ª×—×™× ×©×¨×•×¦×™× ×œ×©×œ×‘ AI ×‘×™×™×©×•××™× ×©×œ×”×
- ğŸ”¬ ×—×•×§×¨×™× ×”×–×§×•×§×™× ×œ×¤×¨×˜×™×•×ª ×•×‘×™×§×•×¨×ª ××œ××”
- ğŸ“ ×¡×˜×•×“× ×˜×™× ×•××ª×œ××“×™× ×©×¨×•×¦×™× ×œ×œ××•×“ ×¢×œ LLMs
- ğŸ¢ ××¨×’×•× ×™× ×¢× ×“×¨×™×©×•×ª ××‘×˜×—×” ××—××™×¨×•×ª
- ğŸ’» ×—×•×‘×‘×™ ×˜×›× ×•×œ×•×’×™×” ×©×¨×•×¦×™× ×œ×”×ª× ×¡×•×ª ×‘××•×“×œ×™× ××ª×§×“××™×

---

## ğŸ’» ×“×¨×™×©×•×ª ××¢×¨×›×ª

### ×“×¨×™×©×•×ª ×—×•××¨×” ××™× ×™××œ×™×•×ª

| ×¨×›×™×‘ | ××™× ×™××•× | ××•××œ×¥ | ××•×¤×˜×™××œ×™ |
|------|---------|-------|----------|
| **RAM** | 8GB | 16GB | 32GB+ |
| **××¢×‘×“** | 4 ×œ×™×‘×•×ª | 8 ×œ×™×‘×•×ª | 16+ ×œ×™×‘×•×ª |
| **××—×¡×•×Ÿ ×¤× ×•×™** | 10GB | 50GB | 100GB+ |
| **×›×¨×˜×™×¡ ××¡×š** | ××™× ×˜×’×¨×˜×™×‘×™ | NVIDIA/AMD 6GB+ | RTX 3090/4090 |

### ğŸ“Š ×”××œ×¦×•×ª ×œ×¤×™ ××•×“×œ

| ×’×•×“×œ ××•×“×œ | RAM × ×“×¨×© | ×“×•×’×××•×ª ×œ××•×“×œ×™× |
|-----------|----------|-----------------|
| **1B-3B** | 8GB | Gemma 3:1b, Llama 3.2:1b, Phi 4 Mini |
| **7B-8B** | 16GB | Llama 3.1, Mistral, DeepSeek-R1 |
| **13B-14B** | 32GB | Phi 4, Llama 2 13B |
| **70B+** | 64GB+ | Llama 3.3 70B, Llama 4:scout |

### ×“×¨×™×©×•×ª ×ª×•×›× ×”

#### ğŸ§ Linux
- ××¢×¨×›×ª ×”×¤×¢×œ×”: Ubuntu 18.04+, Debian 10+, RHEL 8+, Fedora 35+
- Kernel: 4.15+
- curl ××• wget
- (××•×¤×¦×™×•× ×œ×™) Docker 20.10+

#### ğŸ macOS
- ×’×¨×¡×”: macOS 11 Big Sur ×•××¢×œ×”
- ×¢×™×‘×•×“ Intel ××• Apple Silicon (M1/M2/M3)

#### ğŸªŸ Windows
- ×’×¨×¡×”: Windows 10 22H2 / Windows 11
- WSL2 (××•×¤×¦×™×•× ×œ×™ ××š ××•××œ×¥ ×œ××¤×ª×—×™×)

---

## ğŸš€ ×”×ª×§× ×” ×¦×¢×“ ××—×¨ ×¦×¢×“

### ğŸ§ Linux (×©×™×˜×” ××•××œ×¦×ª)

#### ×©×™×˜×” 1: ×”×ª×§× ×” ××•×˜×•××˜×™×ª (××”×™×¨×”)

{% raw %}
```bash
# ×”×•×¨×“×” ×•×”×ª×§× ×” ×‘×¤×§×•×“×” ××—×ª
curl -fsSL https://ollama.com/install.sh | sh
```
{% endraw %}

**××” ×”×¡×§×¨×™×¤×˜ ×¢×•×©×”?**
1. ×–×™×”×•×™ ××•×˜×•××˜×™ ×©×œ ×”×”×¤×¦×” ×•×”××¨×›×™×˜×§×˜×•×¨×”
2. ×”×•×¨×“×ª ×”×’×¨×¡×” ×”××ª××™××” ×‘×™×•×ª×¨
3. ×”×ª×§× ×ª ×©×™×¨×•×ª systemd
4. ×”×¤×¢×œ×” ××•×˜×•××˜×™×ª ×©×œ ×”×©×™×¨×•×ª

#### ×©×™×˜×” 2: ×”×ª×§× ×” ×™×“× ×™×ª (×©×œ×™×˜×” ××œ××”)

{% raw %}
```bash
# 1. ×”×•×¨×“×ª ×§×•×‘×¥ ×”×‘×™× ××¨×™
curl -L https://ollama.com/download/ollama-linux-amd64 -o ollama

# 2. ×”×•×¡×¤×ª ×”×¨×©××•×ª ×”×¨×¦×”
chmod +x ollama

# 3. ×”×¢×‘×¨×” ×œ× ×ª×™×‘ ××¢×¨×›×ª
sudo mv ollama /usr/local/bin/

# 4. ×™×¦×™×¨×ª ××©×ª××© ××™×•×—×“ (××‘×˜×—×” ××•××œ×¦×ª)
sudo useradd -r -s /bin/false -m -d /usr/share/ollama ollama

# 5. ×™×¦×™×¨×ª ×§×•×‘×¥ ×©×™×¨×•×ª systemd
sudo tee /etc/systemd/system/ollama.service > /dev/null <<EOF
[Unit]
Description=Ollama Service
After=network-online.target

[Service]
ExecStart=/usr/local/bin/ollama serve
User=ollama
Group=ollama
Restart=always
RestartSec=3
Environment="PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin"

[Install]
WantedBy=default.target
EOF

# 6. ×”×¤×¢×œ×ª ×”×©×™×¨×•×ª
sudo systemctl daemon-reload
sudo systemctl enable ollama
sudo systemctl start ollama

# 7. ×‘×“×™×§×ª ×¡×˜×˜×•×¡
sudo systemctl status ollama
```
{% endraw %}

#### ğŸ® ×ª××™×›×” ×‘×›×¨×˜×™×¡×™ ××¡×š NVIDIA

{% raw %}
```bash
# ×”×ª×§× ×ª NVIDIA Container Toolkit
distribution=$(. /etc/os-release;echo $ID$VERSION_ID)
curl -s -L https://nvidia.github.io/nvidia-docker/gpgkey | sudo apt-key add -
curl -s -L https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.list | \
  sudo tee /etc/apt/sources.list.d/nvidia-docker.list

sudo apt-get update
sudo apt-get install -y nvidia-container-toolkit

# ××ª×—×•×œ Ollama
sudo systemctl restart ollama
```
{% endraw %}

---

### ğŸ macOS

#### ×©×™×˜×” 1: ×”×ª×§× ×” ×’×¨×¤×™×ª (××•××œ×¥ ×œ××ª×—×™×œ×™×)

1. **×”×•×¨×“×”:**
   {% raw %}
```bash
   # ×¤×ª×™×—×ª ×“×¤×“×¤×Ÿ ×•×”×•×¨×“×”
   open https://ollama.com/download/Ollama.dmg
   ```
{% endraw %}

2. **×”×ª×§× ×”:**
   - ×¤×ª×— ××ª ×§×•×‘×¥ ×”-DMG ×©×”×•×¨×“
   - ×’×¨×•×¨ ××ª ××™×™×§×•×Ÿ Ollama ×œ×ª×™×§×™×™×ª Applications
   - ×¤×ª×— ××ª Ollama ××ª×™×§×™×™×ª Applications
   - ××©×¨ ××ª ×”×”×ª×¨××•×ª ×‘×¤×¢× ×”×¨××©×•× ×”

3. **××™××•×ª:**
   {% raw %}
```bash
   # ×‘×“×™×§×” ×©×”×¤×§×•×“×” ×–××™× ×” ×‘×˜×¨××™× ×œ
   ollama --version
   ```
{% endraw %}

#### ×©×™×˜×” 2: Homebrew (×œ××¤×ª×—×™×)

{% raw %}
```bash
# ×”×ª×§× ×” ×“×¨×š Homebrew
brew install ollama

# ×”×¤×¢×œ×ª ×”×©×™×¨×•×ª
brew services start ollama

# ×‘×“×™×§×ª ×¡×˜×˜×•×¡
brew services list | grep ollama
```
{% endraw %}

#### ğŸ’¡ ×˜×™×¤ ×œ××©×ª××©×™ Apple Silicon

{% raw %}
```bash
# ×‘×“×™×§×ª ×¡×•×’ ×”××¢×‘×“
uname -m

# ×ª×•×¦××”: arm64 = Apple Silicon (M1/M2/M3)
# ×ª×•×¦××”: x86_64 = Intel

# Ollama ××–×”×” ××•×˜×•××˜×™×ª ×•××©×ª××© ×‘××•×¤×˜×™××™×–×¦×™×•×ª ×”××ª××™××•×ª
```
{% endraw %}

---

### ğŸªŸ Windows

#### ×©×™×˜×” 1: ×”×ª×§× ×” ×’×¨×¤×™×ª (××•××œ×¦×ª)

1. **×”×•×¨×“×”:**
   - ×’×© ×œ×›×ª×•×‘×ª: https://ollama.com/download/OllamaSetup.exe
   - ××• ×“×¨×š PowerShell:
   {% raw %}
```powershell
   # ×”×•×¨×“×” ×“×¨×š PowerShell
   Invoke-WebRequest -Uri "https://ollama.com/download/OllamaSetup.exe" -OutFile "$env:TEMP\OllamaSetup.exe"
   
   # ×”×¨×¦×ª ×”×”×ª×§× ×”
   Start-Process "$env:TEMP\OllamaSetup.exe"
   ```{% raw %}
{% endraw %}

2. **×”×ª×§× ×”:**
   - ×”×¨×¥ ××ª {% endraw %}`OllamaSetup.exe`
   - ××©×¨ ××ª User Account Control (UAC)
   - ×¢×§×•×‘ ××—×¨ ××©×£ ×”×”×ª×§× ×”
   - Ollama ×™×ª×•×•×¡×£ ××•×˜×•××˜×™×ª ×œ-PATH

3. **××™××•×ª:**
   {% raw %}
```powershell
   # ×¤×ª×— PowerShell ××• CMD ×—×“×©
   ollama --version
   ```
{% endraw %}

#### ×©×™×˜×” 2: WSL2 (×œ××¤×ª×—×™×)

{% raw %}
```bash
# 1. ×•×•×“× ×©-WSL2 ××•×ª×§×Ÿ
wsl --install

# 2. ×¤×ª×— Ubuntu ××• ×”×¤×¦×” ××—×¨×ª
wsl

# 3. ×”×ª×§×Ÿ Ollama ×‘×ª×•×š WSL
curl -fsSL https://ollama.com/install.sh | sh

# 4. ×”×¨×¥ ×-Windows PowerShell
wsl ollama serve
```
{% endraw %}

#### âš™ï¸ ×”×’×“×¨×•×ª × ×•×¡×¤×•×ª ×œ-Windows

{% raw %}
```powershell
# ×”×•×¡×¤×ª Ollama ×œ×—×•××ª ×”××© (×× × ×“×¨×©)
New-NetFirewallRule -DisplayName "Ollama" -Direction Inbound -Program "C:\Program Files\Ollama\ollama.exe" -Action Allow

# ×©×™× ×•×™ ×™×¦×™××ª ×‘×¨×™×¨×ª ×”××—×“×œ (××•×¤×¦×™×•× ×œ×™)
[Environment]::SetEnvironmentVariable("OLLAMA_HOST", "0.0.0.0:11434", "User")
```
{% endraw %}

---

### ğŸ³ Docker (×›×œ ××¢×¨×›×•×ª ×”×”×¤×¢×œ×”)

#### ×”×ª×§× ×” ×‘×¡×™×¡×™×ª

{% raw %}
```bash
# ×”×¨×¦×ª Ollama ×‘×§×•× ×˜×™×™× ×¨
docker run -d -v ollama:/root/.ollama -p 11434:11434 --name ollama ollama/ollama
```
{% endraw %}

#### ×¢× ×ª××™×›×ª GPU (NVIDIA)

{% raw %}
```bash
# ×”×¨×¦×” ×¢× CUDA
docker run -d \
  --gpus=all \
  -v ollama:/root/.ollama \
  -p 11434:11434 \
  --name ollama \
  ollama/ollama
```{% raw %}
{% endraw %}

#### Docker Compose (××•××œ×¥ ×œ×¤×¨×•×“×§×©×Ÿ)

×¦×•×¨ ×§×•×‘×¥ {% endraw %}`docker-compose.yml`:

{% raw %}
```yaml
version: '3.8'

services:
  ollama:
    image: ollama/ollama:latest
    container_name: ollama
    ports:
      - "11434:11434"
    volumes:
      - ollama_data:/root/.ollama
    restart: unless-stopped
    # ×”×¡×¨ ×”×¢×¨×” ×œ×ª××™×›×ª GPU:
    # deploy:
    #   resources:
    #     reservations:
    #       devices:
    #         - driver: nvidia
    #           count: all
    #           capabilities: [gpu]

volumes:
  ollama_data:
    driver: local
```
{% endraw %}

{% raw %}
```bash
# ×”×¤×¢×œ×”
docker-compose up -d

# ×‘×“×™×§×ª ×œ×•×’×™×
docker-compose logs -f

# ×¢×¦×™×¨×”
docker-compose down
```
{% endraw %}

---

### ğŸ“± Android (Termux) - ××§×¡×¤×¨×™×× ×˜×œ×™

âš ï¸ **×©×™× ×œ×‘:** ×–×•×”×™ ×©×™×˜×” ×œ× ×¨×©××™×ª ×•××ª××™××” ×œ××©×ª××©×™× ××ª×§×“××™× ×‘×œ×‘×“.

{% raw %}
```bash
# 1. ×”×ª×§×Ÿ Termux ×-F-Droid (×œ× Google Play!)
# ×”×•×¨×“ ×: https://f-droid.org/packages/com.termux/

# 2. ×¢×“×›×Ÿ ×—×‘×™×œ×•×ª
pkg update && pkg upgrade

# 3. ×”×ª×§×Ÿ ×ª×œ×•×™×•×ª
pkg install wget proot-distro

# 4. ×”×ª×§×Ÿ Ubuntu ×‘×ª×•×š Termux
proot-distro install ubuntu
proot-distro login ubuntu

# 5. ×›×¢×ª ×¤×¢×œ ×œ×¤×™ ×”×•×¨××•×ª Linux
curl -fsSL https://ollama.com/install.sh | sh

# 6. ×”×¨×¥ ××•×“×œ ×§×˜×Ÿ (××•××œ×¥ gemma3:1b)
ollama run gemma3:1b
```
{% endraw %}

ğŸ’¡ **×”××œ×¦×•×ª ×œ×× ×“×¨×•××™×“:**
- ×”×©×ª××© ×‘××•×“×œ×™× ×§×˜× ×™× (1B-3B)
- ×¡×’×•×¨ ××¤×œ×™×§×¦×™×•×ª ××—×¨×•×ª ×‘×–××Ÿ ×”×©×™××•×©
- ×—×‘×¨ ××ª ×”××›×©×™×¨ ×œ××˜×¢×Ÿ
- ×“×¨×•×© ×œ×¤×—×•×ª 8GB RAM ×‘×˜×œ×¤×•×Ÿ

---

## âš™ï¸ ×”×’×“×¨×” ×¨××©×•× ×™×ª

### ×‘×“×™×§×ª ×”×ª×§× ×” ×ª×§×™× ×”

{% raw %}
```bash
# ×‘×“×™×§×ª ×’×¨×¡×”
ollama --version
# ×¤×œ×˜ ××¦×•×¤×”: ollama version is X.X.X

# ×‘×“×™×§×ª ×—×™×‘×•×¨ ×œ×©×¨×ª
ollama list
# ×¤×œ×˜ ××¦×•×¤×”: ×¨×©×™××” ×¨×™×§×” ××• ××•×“×œ×™× ×§×™×™××™×
```
{% endraw %}

### ×”×•×¨×“×ª ×”××•×“×œ ×”×¨××©×•×Ÿ

{% raw %}
```bash
# ×”×•×¨×“×ª ××•×“×œ ×§×œ ×œ××ª×—×™×œ×™× (Gemma 3 - 4B)
ollama pull gemma3

# ×ª×”×œ×™×š ×”×”×•×¨×“×” ××¦×™×’:
# pulling manifest
# pulling model... 100%
# verifying sha256 digest
# success
```{% raw %}
{% endraw %}

### ğŸ¯ ×‘×—×™×¨×ª ×”××•×“×œ ×”×¨××©×•×Ÿ ×©×œ×š

| ××•×“×œ | ×’×•×“×œ | ×©×™××•×© ××•××œ×¥ | RAM × ×“×¨×© |
|------|------|-------------|----------|
| {% endraw %}`gemma3:1b` | 815MB | âš¡ ×ª×©×•×‘×•×ª ××”×™×¨×•×ª, ×¦'××˜ ×‘×¡×™×¡×™ | 4GB |
| `gemma3` | 3.3GB | ğŸ’¬ ×©×™×—×•×ª ×›×œ×œ×™×•×ª, ×§×•×“ ×¤×©×•×˜ | 8GB |
| `llama3.2` | 2.0GB | ğŸ¯ ××™×–×•×Ÿ ×‘×™×Ÿ ××”×™×¨×•×ª ×œ××™×›×•×ª | 8GB |
| `deepseek-r1` | 4.7GB | ğŸ§  ×—×©×™×‘×” ××ª×§×“××ª, ××ª××˜×™×§×” | 16GB |
| `llama3.2-vision` | 7.9GB | ğŸ‘ï¸ ×¢×™×‘×•×“ ×ª××•× ×•×ª | 16GB |
| `codellama` | 3.8GB | ğŸ’» ×›×ª×™×‘×ª ×§×•×“ | 8GB |

{% raw %}
```bash
# ×“×•×’××”: ×”×•×¨×“×ª ××•×“×œ ×œ×§×•×“
ollama pull codellama

# ×“×•×’××”: ×”×•×¨×“×ª ××•×“×œ ×¨××™×™×”
ollama pull llama3.2-vision
```
{% endraw %}

### ğŸ”§ ××©×ª× ×™ ×¡×‘×™×‘×” (××•×¤×¦×™×•× ×œ×™)

#### Linux/macOS

{% raw %}
```bash
# ×”×•×¡×¤×” ×œ-~/.bashrc ××• ~/.zshrc

# ×©×™× ×•×™ ×›×ª×•×‘×ª ×”×©×¨×ª
export OLLAMA_HOST="0.0.0.0:11434"

# ××™×§×•× ××—×¡×•×Ÿ ×”××•×“×œ×™×
export OLLAMA_MODELS="/path/to/models"

# ×”×’×‘×œ×ª ×–×™×›×¨×•×Ÿ GPU (MB)
export OLLAMA_MAX_VRAM=4096

# ×”×—×œ×ª ×”×©×™× ×•×™×™×
source ~/.bashrc
```
{% endraw %}

#### Windows (PowerShell)

{% raw %}
```powershell
# ×”×’×“×¨×ª ××©×ª× ×™× ×§×‘×•×¢×™×
[Environment]::SetEnvironmentVariable("OLLAMA_HOST", "0.0.0.0:11434", "User")
[Environment]::SetEnvironmentVariable("OLLAMA_MODELS", "C:\ollama-models", "User")

# ×”×¤×¢×œ×” ××—×“×© ×©×œ PowerShell
```
{% endraw %}

---

## ğŸ® ×©×™××•×© ×‘×¡×™×¡×™

### 1ï¸âƒ£ ××¦×‘ ××™× ×˜×¨××§×˜×™×‘×™ (×¦'××˜)

{% raw %}
```bash
# ×”×¨×¦×ª ××•×“×œ ×‘××¦×‘ ×©×™×—×”
ollama run gemma3

# ×“×•×’××” ×œ×©×™×—×”:
# >>> ×”×™×™! ×¡×¤×¨ ×œ×™ ×‘×§×¦×¨×” ××” ×–×” ×œ××™×“×ª ××›×•× ×”
# 
# ×œ××™×“×ª ××›×•× ×” ×”×™× ×ª×—×•× ×‘×‘×™× ×” ××œ×›×•×ª×™×ª ×©×‘×• ××—×©×‘×™× ×œ×•××“×™×
# ×× ×ª×•× ×™× ××‘×œ×™ ×œ×”×™×•×ª ××ª×•×›× ×ª×™× ×‘××¤×•×¨×©...
#
# >>> /bye
```{% raw %}
{% endraw %}

#### ×¤×§×•×“×•×ª ×©×™××•×©×™×•×ª ×‘××¦×‘ ××™× ×˜×¨××§×˜×™×‘×™

| ×¤×§×•×“×” | ×ª×™××•×¨ |
|-------|-------|
| {% endraw %}`/bye` | ×™×¦×™××” ××”×¦'××˜ |
| `/clear` | × ×™×§×•×™ ×”×™×¡×˜×•×¨×™×™×ª ×”×©×™×—×” |
| `/set parameter value` | ×©×™× ×•×™ ×¤×¨××˜×¨×™× |
| `/show info` | ×”×¦×’×ª ××™×“×¢ ×¢×œ ×”××•×“×œ |
| `/multiline` | ×›×ª×™×‘×ª ×¤×¡×§×” ××¨×•×‘×ª ×©×•×¨×•×ª |

### 2ï¸âƒ£ ×©××™×œ×ª×” ×—×“-×¤×¢××™×ª

{% raw %}
```bash
# ×©××™×œ×ª×” ×™×©×™×¨×” ×œ×œ× ×›× ×™×¡×” ×œ××¦×‘ ××™× ×˜×¨××§×˜×™×‘×™
ollama run gemma3 "×›×ª×•×‘ ×œ×™ ×¤×•× ×§×¦×™×” ×¤×™×™×ª×•×Ÿ ×©××—×©×‘×ª ×¤×™×‘×•× ××¦'×™"

# ×¢× ×¤×¨××˜×¨×™× ××•×ª×××™× ××™×©×™×ª
ollama run gemma3 --temperature 0.7 --top-p 0.9 \
  "×”×¡×‘×¨ ××ª ×ª×•×¨×ª ×”×™×—×¡×•×ª ×©×œ ××™×™× ×©×˜×™×™×Ÿ ×‘×¤×©×˜×•×ª"
```
{% endraw %}

### 3ï¸âƒ£ ×¢×‘×•×“×” ×¢× ×§×‘×¦×™×

{% raw %}
```bash
# ×”×¢×‘×¨×ª ×§×•×‘×¥ ×˜×§×¡×˜ ×œ××•×“×œ
cat document.txt | ollama run gemma3 "×¡×›× ××ª ×”×˜×§×¡×˜ ×”×‘×:"

# × ×™×ª×•×— ×§×•×“
ollama run codellama "×¡×§×•×¨ ××ª ×”×§×•×“ ×”×‘× ×•××¦× ×‘××’×™×: $(cat script.py)"
```
{% endraw %}

### 4ï¸âƒ£ ×¢×‘×•×“×” ×¢× ×ª××•× ×•×ª (××•×“×œ×™ Vision)

{% raw %}
```bash
# × ×™×ª×•×— ×ª××•× ×”
ollama run llama3.2-vision "××” ××ª×” ×¨×•××” ×‘×ª××•× ×” ×”×–×•?" --image path/to/image.jpg

# ×ª×™××•×¨ ××¤×•×¨×˜
ollama run llama3.2-vision \
  "×ª××¨ ××ª ×”×ª××•× ×” ×‘×¤×™×¨×•×˜, ×›×•×œ×œ ×¦×‘×¢×™×, ××•×‘×™×™×§×˜×™× ×•×¨×§×¢" \
  --image photo.png
```
{% endraw %}

### 5ï¸âƒ£ API REST - ×©×™××•×© ××ª×•×›× ×™×•×ª

#### Python

{% raw %}
```python
import requests
import json

# ×©×œ×™×—×ª ×‘×§×©×” ×œ××•×“×œ
url = "http://localhost:11434/api/generate"
payload = {
    "model": "gemma3",
    "prompt": "××” ×‘×™×¨×ª ×™×©×¨××œ?",
    "stream": False
}

response = requests.post(url, json=payload)
result = response.json()
print(result['response'])
```
{% endraw %}

#### ×“×•×’××” ××ª×§×“××ª ×¢× Streaming

{% raw %}
```python
import requests

url = "http://localhost:11434/api/generate"
payload = {
    "model": "gemma3",
    "prompt": "×›×ª×•×‘ ×¡×™×¤×•×¨ ×§×¦×¨ ×¢×œ ×¨×•×‘×•×˜",
    "stream": True
}

with requests.post(url, json=payload, stream=True) as response:
    for line in response.iter_lines():
        if line:
            data = json.loads(line)
            print(data.get('response', ''), end='', flush=True)
```
{% endraw %}

#### JavaScript/Node.js

{% raw %}
```javascript
const ollama = require('ollama');

async function chat() {
  const response = await ollama.generate({
    model: 'gemma3',
    prompt: '××” ×©×œ×•××š?'
  });
  
  console.log(response.response);
}

chat();
```
{% endraw %}

#### cURL (×‘×“×™×§×•×ª ××”×™×¨×•×ª)

{% raw %}
```bash
# ×‘×§×©×” ×¤×©×•×˜×”
curl http://localhost:11434/api/generate -d '{
  "model": "gemma3",
  "prompt": "×œ××” ×”×©××™×™× ×›×—×•×œ×™×?",
  "stream": false
}'

# ×¢× ×¤×¨××˜×¨×™× ××ª×§×“××™×
curl http://localhost:11434/api/generate -d '{
  "model": "gemma3",
  "prompt": "×›×ª×•×‘ ×©×™×¨ ×¢×œ ×”×—×™×™×",
  "stream": false,
  "options": {
    "temperature": 0.8,
    "top_p": 0.9,
    "top_k": 40
  }
}'
```
{% endraw %}

---

## ğŸ”¥ ×˜×™×¤×™× ××ª×§×“××™×

### 1. ×™×¦×™×¨×ª Modelfile ××•×ª×× ××™×©×™×ª

Modelfile ×××¤×©×¨ ×œ×š ×œ×”×ª××™× ××•×“×œ ×§×™×™× ×œ×”×ª× ×”×’×•×ª ×¡×¤×¦×™×¤×™×ª.

{% raw %}
```{% raw %}dockerfile
# ×¦×•×¨ ×§×•×‘×¥ ×‘×©× Modelfile

FROM gemma3

# ×”×’×“×¨×ª ×˜××¤×¨×˜×•×¨×” - ×©×•×œ×˜ ×‘×™×¦×™×¨×ª×™×•×ª
PARAMETER temperature 0.9

# ×”×•×¨××•×ª ××¢×¨×›×ª - ××’×“×™×¨×•×ª ××ª ××•×¤×™ ×”××•×“×œ
SYSTEM """
××ª×” ×¢×•×–×¨ ×˜×›× ×™ ××•××—×” ×‘×ª×—×•× ×”×¤×™×™×ª×•×Ÿ.
×ª×Ÿ ×ª×©×•×‘×•×ª ×§×¦×¨×•×ª ×•×××•×§×“×•×ª ×¢× ×“×•×’×××•×ª ×§×•×“.
×”×©×ª××© ×‘×ª×—×‘×™×¨ markdown ×œ×¢×™×¦×•×‘.
"""

# ×”×•×“×¢×ª ×¤×ª×™×—×”
TEMPLATE """{{ if .System }}<|system|>
{{ .System }}<|end|>
{{ end }}{{ if .Prompt }}<|user|>
{{ .Prompt }}<|end|>
{{ end }}<|assistant|>
"""
{% endraw %}```
{% endraw %}

{% raw %}
```bash
# ×™×¦×™×¨×ª ×”××•×“×œ ×”××•×ª××
ollama create python-expert -f Modelfile

# ×”×¨×¦×”
ollama run python-expert "××™×š ×œ×™×¦×•×¨ virtual environment?"
```
{% endraw %}

### 2. ×™×™×‘×•× ××•×“×œ×™× ×-Hugging Face (GGUF)

{% raw %}
```{% raw %}bash
# 1. ×”×•×¨×“ ××•×“×œ GGUF ×-Hugging Face
wget https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.2-GGUF/resolve/main/mistral-7b-instruct-v0.2.Q4_K_M.gguf

# 2. ×¦×•×¨ Modelfile
cat > Modelfile << 'EOF'
FROM ./mistral-7b-instruct-v0.2.Q4_K_M.gguf

PARAMETER stop "<|im_end|>"
PARAMETER stop "<|im_start|>"

TEMPLATE """<|im_start|>system
{{ .System }}<|im_end|>
<|im_start|>user
{{ .Prompt }}<|im_end|>
<|im_start|>assistant
"""
EOF

# 3. ×™×¦×™×¨×ª ×”××•×“×œ
ollama create my-mistral -f Modelfile

# 4. ×”×¨×¦×”
ollama run my-mistral
{% endraw %}```
{% endraw %}

### 3. ××•×¤×˜×™××™×–×¦×™×” ×œ××•×“×œ×™× ×’×“×•×œ×™×

#### Quantization - ×”×§×˜× ×ª ×’×•×“×œ ×”××•×“×œ

{% raw %}
```bash
# ×”×•×¨×“×ª ××•×“×œ ×‘×§×•×•× ×˜×™×–×¦×™×•×ª ×©×•× ×•×ª

# Q4_0 - ×”×›×™ ×§×˜×Ÿ, ×”×›×™ ××”×™×¨, ××™×›×•×ª ×‘×™× ×•× ×™×ª
ollama pull llama3.1:8b-q4_0

# Q5_K_M - ××™×–×•×Ÿ ×˜×•×‘
ollama pull llama3.1:8b-q5_k_m

# Q8_0 - ××™×›×•×ª ×’×‘×•×”×”, ××™×˜×™ ×™×•×ª×¨
ollama pull llama3.1:8b-q8_0

# ×”×©×•×•××ª ×’×“×œ×™×
ollama list
```
{% endraw %}

#### Context Window - ×”×’×“×œ×ª ×–×™×›×¨×•×Ÿ ×”×©×™×—×”

{% raw %}
```bash
# ×”×¨×¦×” ×¢× ×—×œ×•×Ÿ ×”×§×©×¨ ××•×’×“×œ (×‘×¨×™×¨×ª ××—×“×œ: 2048)
ollama run gemma3 --ctx-size 4096

# ××• ×‘-Modelfile:
# PARAMETER num_ctx 4096
```
{% endraw %}

### 4. ×’×™×©×” ××¨×—×•×§ ×œ×©×¨×ª Ollama

#### ×”×’×“×¨×ª ×©×¨×ª

{% raw %}
```bash
# Linux/macOS - ×¢×¨×•×š ××ª ×§×•×‘×¥ ×”×©×™×¨×•×ª
sudo systemctl edit ollama

# ×”×•×¡×£:
[Service]
Environment="OLLAMA_HOST=0.0.0.0:11434"

# ××ª×—×œ
sudo systemctl restart ollama

# Windows - ×¤×ª×— PowerShell ×›×× ×”×œ
[Environment]::SetEnvironmentVariable("OLLAMA_HOST", "0.0.0.0:11434", "Machine")
Restart-Service Ollama
```
{% endraw %}

#### ×—×™×‘×•×¨ ××œ×§×•×—

{% raw %}
```bash
# ×‘××—×©×‘ ××¨×•×—×§
export OLLAMA_HOST="http://192.168.1.100:11434"
ollama list

# ×‘×§×©×ª API ××¨×—×•×§
curl http://192.168.1.100:11434/api/generate -d '{
  "model": "gemma3",
  "prompt": "Hello from remote!"
}'
```
{% endraw %}

### 5. Multi-modal - ×©×™×œ×•×‘ ×˜×§×¡×˜ ×•×ª××•× ×•×ª

{% raw %}
```python
import base64
import requests

# ×§×¨×™××ª ×ª××•× ×”
with open("image.jpg", "rb") as f:
    image_data = base64.b64encode(f.read()).decode()

# ×©×œ×™×—×” ×œ××•×“×œ
url = "http://localhost:11434/api/generate"
payload = {
    "model": "llama3.2-vision",
    "prompt": "×ª××¨ ××” ×™×© ×‘×ª××•× ×” ×•××¦× ××•×‘×™×™×§×˜×™× ×—×©×•×“×™×",
    "images": [image_data],
    "stream": False
}

response = requests.post(url, json=payload)
print(response.json()['response'])
```
{% endraw %}

### 6. Batch Processing - ×¢×™×‘×•×“ ××¦×•×•×”

{% raw %}
```bash
#!/bin/bash
# ×¢×™×‘×•×“ ×¨×©×™××ª ×©××œ×•×ª ××§×•×‘×¥

while IFS= read -r question; do
    echo "×©××œ×”: $question"
    echo "×ª×©×•×‘×”:"
    ollama run gemma3 "$question" --nowordwrap
    echo "---"
done < questions.txt > answers.txt
```
{% endraw %}

### 7. ×©×™××•×© ×‘-GPU ××¨×•×‘×™×

{% raw %}
```bash
# ×”×¦×’×ª GPU ×–××™× ×™×
nvidia-smi

# ×”×’×“×¨×ª ××©×ª× ×™ ×¡×‘×™×‘×”
export CUDA_VISIBLE_DEVICES=0,1  # ×©×™××•×© ×‘-GPU 0 ×•-1

# ××• ×‘×”×¨×¦×” ×¡×¤×¦×™×¤×™×ª
CUDA_VISIBLE_DEVICES=1 ollama run llama3.3:70b
```
{% endraw %}

### 8. Prompt Engineering - ×˜×›× ×™×§×•×ª ××ª×§×“××•×ª

#### Chain of Thought (×—×©×™×‘×” ×©×œ×‘×™×ª)

{% raw %}
```bash
ollama run gemma3 "
×¤×ª×•×¨ ××ª ×”×‘×¢×™×” ×”×‘××” ×¦×¢×“ ××—×¨ ×¦×¢×“:

×× ×œ×¨×›×‘×ª ×™×© 15 ×§×¨×•× ×•×ª, ×•×‘×›×œ ×§×¨×•×Ÿ 28 ××•×©×‘×™×,
×•-85% ××”××•×©×‘×™× ×ª×¤×•×¡×™×, ×›××” × ×•×¡×¢×™× ×™×© ×‘×¨×›×‘×ª?

×”×¨××” ××ª ×”×ª×”×œ×™×š:
1. ×—×™×©×•×‘ ×¡×”\"×š ××•×©×‘×™×
2. ×—×™×©×•×‘ ××•×©×‘×™× ×ª×¤×•×¡×™×
3. ×ª×©×•×‘×” ×¡×•×¤×™×ª
"
```{% raw %}
{% endraw %}

#### Few-Shot Learning (×œ××™×“×” ××“×•×’×××•×ª)

{% endraw %}```bash
ollama run gemma3 "
×ª×¨×’× ××©×¤×˜×™× ×œ×× ×’×œ×™×ª:

×“×•×’××” 1:
×¢×‘×¨×™×ª: ×©×œ×•× ×¢×•×œ×
×

---

## ğŸ“Š ×¡×˜×˜×™×¡×˜×™×§×•×ª ×”×¤×¨×•×™×§×˜

- **×›×•×›×‘×™×**: 157,060 â­
- **Forks**: 13,828 ğŸ”±
- **Issues**: 2,316 ğŸ›
- **×©×¤×”**: Go ğŸ’»
- **×¨×™×©×™×•×Ÿ**: MIT License ğŸ“œ

## ğŸ”— ×§×™×©×•×¨×™× ×©×™××•×©×™×™×

- [×¨×™×¤×• ×‘-GitHub](https://github.com/ollama/ollama)
- [Issues & ×ª××™×›×”](https://github.com/ollama/ollama/issues)
- [Discussions](https://github.com/ollama/ollama/discussions)
- [Wiki](https://github.com/ollama/ollama/wiki)

---

*××“×¨×™×š ×–×” × ×•×¦×¨ ××•×˜×•××˜×™×ª ×¢×œ ×™×“×™ AI Guide Bot ×¢× Claude AI*
*×¢×“×›×•×Ÿ ××—×¨×•×Ÿ: 04/12/2025 12:26*
