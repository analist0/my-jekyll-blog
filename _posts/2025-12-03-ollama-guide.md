---
layout: unified-post
title: "××“×¨×™×š ××§×¦×•×¢×™: ollama - Get up and running with OpenAI gpt-oss, DeepSeek-R1, Gemma 3 and other models."
date: 2025-12-03 19:13:07 +0200
categories: [AI, LLM, ××“×¨×™×›×™×]
tags: [local-ai, llm, installation, go]
image: /assets/images/repos/ollama-20251203.png
author: AI Guide Bot
lang: he
dir: rtl
---

![ollama](/assets/images/repos/ollama-20251203.png)

# ğŸš€ ollama

**â­ 156,999 ×›×•×›×‘×™× | ğŸ”§ Go | ğŸ“… ×¢×“×›×•×Ÿ ××—×¨×•×Ÿ: 2025-12-03**

[ğŸ”— ×§×™×©×•×¨ ×œ×¨×™×¤×•](https://github.com/ollama/ollama) | [â¬‡ï¸ ×”×•×¨×“×”](https://github.com/ollama/ollama/archive/refs/heads/main.zip)

---

# ğŸ“š ××“×¨×™×š ×”×ª×§× ×” ××§×™×£ ×œ-Ollama - ×”×¨×¦×ª ××•×“×œ×™ AI ××§×•××™×ª

## ğŸ¯ ×¡×§×™×¨×” ×›×œ×œ×™×ª

**Ollama** ×”×™× ×¤×œ×˜×¤×•×¨××” ×§×œ×” ×•×™×¢×™×œ×” ×œ×”×¨×¦×ª ××•×“×œ×™ ×©×¤×” ×’×“×•×œ×™× (LLMs) ×‘××•×¤×Ÿ ××§×•××™ ×¢×œ ×”××—×©×‘ ×©×œ×š. ×‘××§×•× ×œ×”×¡×ª××š ×¢×œ ×©×™×¨×•×ª×™ ×¢× ×Ÿ ×™×§×¨×™×, Ollama ×××¤×©×¨×ª ×œ×š ×œ×”×¨×™×¥ ××•×“×œ×™× ×›××• Llama, Gemma, DeepSeek-R1 ×•×¢×•×“ ×™×©×™×¨×•×ª ×¢×œ ×”××›×•× ×” ×©×œ×š.

### ğŸŒŸ ×œ××” Ollama?

- **ğŸ”’ ×¤×¨×˜×™×•×ª ××œ××”** - ×”× ×ª×•× ×™× ×©×œ×š × ×©××¨×™× ××¦×œ×š ×‘××—×©×‘
- **ğŸ’° ×—×™× × ×œ×—×œ×•×˜×™×Ÿ** - ×œ×œ× ×¢×œ×•×™×•×ª API ××• ×× ×•×™×™× ×—×•×“×©×™×™×
- **âš¡ ××”×™×¨×•×ª** - ××™×Ÿ ×ª×œ×•×ª ×‘×¨×©×ª, ×ª×’×•×‘×•×ª ××™×™×“×™×•×ª
- **ğŸ¨ ×”×ª×××” ××™×©×™×ª** - ×™×›×•×œ×ª ×œ×›×•×•× ×Ÿ ×•×œ×”×ª××™× ××•×“×œ×™× ×œ×¦×¨×›×™× ×©×œ×š
- **ğŸ”Œ ×¢×‘×•×“×” ××•×¤×œ×™×™×Ÿ** - ×¤×•×¢×œ ×’× ×œ×œ× ×—×™×‘×•×¨ ×œ××™× ×˜×¨× ×˜

---

## ğŸ’» ×“×¨×™×©×•×ª ××¢×¨×›×ª

### ×“×¨×™×©×•×ª ××™× ×™××•×:

| ×¨×›×™×‘ | ×“×¨×™×©×” |
|------|-------|
| **RAM** | 8GB (×œ××•×“×œ×™× ×©×œ 7B ×¤×¨××˜×¨×™×) |
| **××—×¡×•×Ÿ** | 10GB ×¤× ×•×™×™× ×œ×¤×—×•×ª |
| **××¢×‘×“** | ××¢×‘×“ ××•×“×¨× ×™ (Intel i5/AMD Ryzen 5 ×•××¢×œ×”) |
| **××¢×¨×›×ª ×”×¤×¢×œ×”** | Windows 10+, macOS 11+, ××• Linux |

### ×“×¨×™×©×•×ª ××•××œ×¦×•×ª:

| ×’×•×“×œ ××•×“×œ | RAM ××•××œ×¥ | ×“×•×’××” |
|-----------|-----------|--------|
| 1B-7B | 8-16GB | Gemma 3:1b, Llama 3.2 |
| 13B-27B | 16-32GB | Gemma 3:27b, Phi 4 |
| 33B-70B | 32-64GB | Llama 3.3, QwQ |
| 100B+ | 64GB+ | Llama 4, DeepSeek-R1:671b |

âš ï¸ **×”×¢×¨×” ×—×©×•×‘×”**: ××•×“×œ×™× ×’×“×•×œ×™× ×™×•×ª×¨ ×“×•×¨×©×™× ×™×•×ª×¨ ×–×™×›×¨×•×Ÿ. ×× ××™×Ÿ ×œ×š ××¡×¤×™×§ RAM, ×”××¢×¨×›×ª ×ª×©×ª××© ×‘-swap ×•×ª×”×™×” ××™×˜×™×ª ×××•×“.

ğŸ’¡ **×˜×™×¤**: ×× ×™×© ×œ×š ×›×¨×˜×™×¡ ××¡×š NVIDIA, Ollama ×ª×©×ª××© ×‘×• ××•×˜×•××˜×™×ª ×œ×”××¦×ª ×”×—×™×©×•×‘×™×.

---

## ğŸš€ ×”×ª×§× ×” ×¦×¢×“ ××—×¨ ×¦×¢×“

### ğŸ§ Linux

#### ×©×™×˜×” 1: ×”×ª×§× ×” ××•×˜×•××˜×™×ª (××•××œ×¥)

```bash
# ×”×•×¨×“×” ×•×”×ª×§× ×” ×‘×¤×§×•×“×” ××—×ª
curl -fsSL https://ollama.com/install.sh | sh
```

×”×ª×¡×¨×™×˜ ×™×‘×¦×¢ ××ª ×”×¤×¢×•×œ×•×ª ×”×‘××•×ª:
- âœ… ×”×•×¨×“×ª ×”×§×‘×¦×™× ×”×“×¨×•×©×™×
- âœ… ×”×ª×§× ×ª Ollama ×‘-`/usr/local/bin`
- âœ… ×™×¦×™×¨×ª ×©×™×¨×•×ª systemd
- âœ… ×”×¤×¢×œ×ª ×”×©×™×¨×•×ª ××•×˜×•××˜×™×ª

#### ×©×™×˜×” 2: ×”×ª×§× ×” ×™×“× ×™×ª

```bash
# Ubuntu/Debian
sudo apt update
sudo apt install curl

# ×”×•×¨×“×ª ×”×§×•×‘×¥
curl -L https://ollama.com/download/ollama-linux-amd64 -o ollama
chmod +x ollama
sudo mv ollama /usr/local/bin/

# ×™×¦×™×¨×ª ×©×™×¨×•×ª systemd
sudo useradd -r -s /bin/false -m -d /usr/share/ollama ollama

# ×™×¦×™×¨×ª ×§×•×‘×¥ ×”×©×™×¨×•×ª
sudo tee /etc/systemd/system/ollama.service > /dev/null <<EOF
[Unit]
Description=Ollama Service
After=network-online.target

[Service]
ExecStart=/usr/local/bin/ollama serve
User=ollama
Group=ollama
Restart=always
RestartSec=3

[Install]
WantedBy=default.target
EOF

# ×”×¤×¢×œ×ª ×”×©×™×¨×•×ª
sudo systemctl daemon-reload
sudo systemctl enable ollama
sudo systemctl start ollama
```

#### ×‘×“×™×§×ª ×”×”×ª×§× ×”:

```bash
# ×‘×“×™×§×ª ×’×¨×¡×”
ollama --version

# ×‘×“×™×§×ª ×¡×˜×˜×•×¡ ×”×©×™×¨×•×ª
systemctl status ollama
```

---

### ğŸ macOS

#### ×”×ª×§× ×”:

1. **×”×•×¨×“ ××ª ×”×§×•×‘×¥**:
   - ×’×© ×œ-[https://ollama.com/download/Ollama.dmg](https://ollama.com/download/Ollama.dmg)
   - ××• ×”×©×ª××© ×‘-Terminal:

```bash
# ×”×•×¨×“×” ×¢× curl
curl -L https://ollama.com/download/Ollama.dmg -o ~/Downloads/Ollama.dmg
```

2. **×”×ª×§×Ÿ ××ª ×”××¤×œ×™×§×¦×™×”**:
   - ×¤×ª×— ××ª ×§×•×‘×¥ ×”-DMG ×©×”×•×¨×“×ª
   - ×’×¨×•×¨ ××ª Ollama.app ×œ×ª×™×§×™×™×ª Applications
   - ×¤×ª×— ××ª Ollama ××ª×™×§×™×™×ª Applications

3. **×”×¤×¢×œ×” ×¨××©×•× ×™×ª**:
   - ×‘×”×¤×¢×œ×” ×”×¨××©×•× ×”, macOS ×¢×©×•×™ ×œ×‘×§×© ××™×©×•×¨ (System Settings â†’ Privacy & Security)
   - Ollama ×ª×¨×•×¥ ×›××¤×œ×™×§×¦×™×™×ª ×¨×§×¢ ×‘×©×•×¨×ª ×”×ª×¤×¨×™×˜×™×

#### ×‘×“×™×§×ª ×”×ª×§× ×”:

```bash
# ×¤×ª×— Terminal ×•×‘×“×•×§
ollama --version

# ×× ×”×¤×§×•×“×” ×œ× × ××¦××ª, ×”×•×¡×£ ×œ-PATH:
echo 'export PATH="/Applications/Ollama.app/Contents/MacOS:$PATH"' >> ~/.zshrc
source ~/.zshrc
```

---

### ğŸªŸ Windows

#### ×”×ª×§× ×”:

1. **×”×•×¨×“ ××ª ×”××ª×§×™×Ÿ**:
   - ×’×© ×œ-[https://ollama.com/download/OllamaSetup.exe](https://ollama.com/download/OllamaSetup.exe)

2. **×”×¤×¢×œ ××ª ×”××ª×§×™×Ÿ**:
   - ×”×¤×¢×œ ××ª `OllamaSetup.exe`
   - ×¢×§×•×‘ ××—×¨ ×”×”×•×¨××•×ª ×¢×œ ×”××¡×š
   - ××™×©×•×¨ ××“××™×Ÿ × ×“×¨×©

3. **××—×¨×™ ×”×”×ª×§× ×”**:
   - Ollama ×ª×¨×•×¥ ××•×˜×•××˜×™×ª ×›×©×™×¨×•×ª ×¨×§×¢
   - ×¡××œ ×™×•×¤×™×¢ ×‘××’×© ×”××¢×¨×›×ª (System Tray)

#### ×‘×“×™×§×ª ×”×ª×§× ×”:

×¤×ª×— **PowerShell** ××• **Command Prompt**:

```powershell
# ×‘×“×™×§×ª ×’×¨×¡×”
ollama --version

# ×‘×“×™×§×ª ×©×”×©×™×¨×•×ª ×¨×¥
ollama list
```

#### ×¤×ª×¨×•×Ÿ ×‘×¢×™×•×ª Windows:

×× ×”×¤×§×•×“×” `ollama` ×œ× × ××¦××ª:

```powershell
# ×”×•×¡×£ ×œ-PATH (PowerShell ×›××“××™×Ÿ)
$env:Path += ";C:\Users\$env:USERNAME\AppData\Local\Programs\Ollama"
[Environment]::SetEnvironmentVariable("Path", $env:Path, [System.EnvironmentVariableScope]::User)
```

---

### ğŸ³ Docker

#### ×”×¨×¦×” ×‘×¡×™×¡×™×ª:

```bash
# ×”×¨×¦×ª Ollama ×‘-Docker
docker run -d -v ollama:/root/.ollama -p 11434:11434 --name ollama ollama/ollama
```

#### ×¢× ×ª××™×›×ª GPU (NVIDIA):

```bash
# ×”×ª×§× ×ª NVIDIA Container Toolkit (×¤×¢× ××—×ª)
distribution=$(. /etc/os-release;echo $ID$VERSION_ID)
curl -s -L https://nvidia.github.io/nvidia-docker/gpgkey | sudo apt-key add -
curl -s -L https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.list | sudo tee /etc/apt/sources.list.d/nvidia-docker.list

sudo apt-get update
sudo apt-get install -y nvidia-container-toolkit
sudo systemctl restart docker

# ×”×¨×¦×ª Ollama ×¢× GPU
docker run -d --gpus=all -v ollama:/root/.ollama -p 11434:11434 --name ollama ollama/ollama
```

#### ×©×™××•×© ×¢× Docker Compose:

×¦×•×¨ ×§×•×‘×¥ `docker-compose.yml`:

```yaml
version: '3.8'

services:
  ollama:
    image: ollama/ollama:latest
    container_name: ollama
    ports:
      - "11434:11434"
    volumes:
      - ollama_data:/root/.ollama
    # ×”×¡×¨ ××ª ×”×”×¢×¨×” ×× ×™×© GPU
    # deploy:
    #   resources:
    #     reservations:
    #       devices:
    #         - driver: nvidia
    #           count: all
    #           capabilities: [gpu]
    restart: unless-stopped

volumes:
  ollama_data:
```

×”×¤×¢×œ×”:

```bash
docker-compose up -d
```

---

### ğŸ“± Termux (Android)

âš ï¸ **××–×”×¨×”**: ×”×¨×¦×ª LLMs ×¢×œ Android ×”×™× ××§×¡×¤×¨×™×× ×˜×œ×™×ª ×•××ª××™××” ×¨×§ ×œ××•×“×œ×™× ×§×˜× ×™× ×××•×“.

```bash
# ×”×ª×§× ×ª Termux ×-F-Droid (×œ× ×-Play Store!)
# ××– ×¤×ª×— ××ª Termux ×•×”×¨×¥:

# ×¢×“×›×•×Ÿ ×—×‘×™×œ×•×ª
pkg update && pkg upgrade -y

# ×”×ª×§× ×ª ×ª×œ×•×™×•×ª
pkg install wget proot-distro -y

# ×”×ª×§× ×ª Ubuntu ×‘×ª×•×š Termux
proot-distro install ubuntu
proot-distro login ubuntu

# ×›×¢×ª ××ª×” ×‘×¡×‘×™×‘×ª Ubuntu, ×”×ª×§×Ÿ Ollama:
apt update
apt install curl -y
curl -fsSL https://ollama.com/install.sh | sh

# ×”×¨×¦×ª Ollama
ollama serve &
```

ğŸ’¡ **×˜×™×¤**: ×¢×œ Android, ×”×©×ª××© ×‘××•×“×œ×™× ×§×˜× ×™× ×‘×œ×‘×“ (1B-3B ×¤×¨××˜×¨×™×):
```bash
ollama run gemma3:1b
ollama run llama3.2:1b
```

---

## âš™ï¸ ×”×’×“×¨×” ×¨××©×•× ×™×ª

### 1ï¸âƒ£ ×”×•×¨×“×ª ×”××•×“×œ ×”×¨××©×•×Ÿ

××—×¨×™ ×”×”×ª×§× ×”, ×”×•×¨×“ ××•×“×œ ×›×“×™ ×œ×”×ª×—×™×œ:

```bash
# ××•×“×œ ×§×œ ×œ××ª×—×™×œ×™× (815MB)
ollama pull gemma3:1b

# ××• ××•×“×œ ×™×•×ª×¨ ××ª×§×“× (4.7GB)
ollama pull llama3.1
```

×”×¤×§×•×“×” `pull` ××•×¨×™×“×” ××ª ×”××•×“×œ ××”×©×¨×ª ×©×œ Ollama ×•×©×•××¨×ª ××•×ª×• ××§×•××™×ª.

### 2ï¸âƒ£ ×‘×“×™×§×ª ×”××•×“×œ×™× ×”××•×ª×§× ×™×

```bash
# ×”×¦×’×ª ×›×œ ×”××•×“×œ×™× ×©×”×•×¨×“×ª
ollama list
```

×¤×œ×˜ ×œ×“×•×’××”:
```
NAME                ID              SIZE      MODIFIED
gemma3:1b          a1b2c3d4e5f6    815 MB    2 minutes ago
llama3.1:latest    f6e5d4c3b2a1    4.7 GB    5 minutes ago
```

### 3ï¸âƒ£ ×”×¨×¦×ª ××•×“×œ ×¨××©×•×Ÿ

```bash
# ×”×¨×¦×ª ××•×“×œ ×‘××¦×‘ ××™× ×˜×¨××§×˜×™×‘×™
ollama run gemma3:1b
```

××ª×” ×ª×¨××” prompt ×©×‘×• ×ª×•×›×œ ×œ×”×§×œ×™×“ ×©××œ×•×ª:

```
>>> ××”×™ ×‘×™× ×” ××œ××›×•×ª×™×ª?
×‘×™× ×” ××œ××›×•×ª×™×ª (AI) ×”×™× ×ª×—×•× ×‘××“×¢×™ ×”××—×©×‘ ×”×¢×•×¡×§ ×‘×™×¦×™×¨×ª ××¢×¨×›×•×ª
××—×©×‘ ×”××¡×•×’×œ×•×ª ×œ×‘×¦×¢ ××©×™××•×ª ×”×“×•×¨×©×•×ª ×‘×“×¨×š ×›×œ×œ ××™× ×˜×œ×™×’× ×¦×™×” ×× ×•×©×™×ª...

>>> /bye
```

### 4ï¸âƒ£ ×”×’×“×¨×•×ª ×¡×‘×™×‘×” ××ª×§×“××•×ª

#### ×©×™× ×•×™ ×ª×™×§×™×™×ª ×”××—×¡×•×Ÿ:

```bash
# Linux/Mac
export OLLAMA_MODELS="/path/to/custom/models"
echo 'export OLLAMA_MODELS="/path/to/custom/models"' >> ~/.bashrc

# Windows (PowerShell)
$env:OLLAMA_MODELS = "D:\OllamaModels"
[Environment]::SetEnvironmentVariable("OLLAMA_MODELS", "D:\OllamaModels", "User")
```

#### ×”×’×“×¨×ª ×–×™×›×¨×•×Ÿ GPU:

```bash
# ×”×’×‘×œ×ª ×©×™××•×© ×‘-VRAM (×‘××§×¨×” ×©×œ ××¡×¤×¨ GPUs)
export OLLAMA_NUM_GPU=1
export OLLAMA_GPU_LAYERS=35  # ×›××” ×©×›×‘×•×ª ×œ×”×¢×‘×™×¨ ×œ-GPU
```

#### ×©×™× ×•×™ ×¤×•×¨×˜:

```bash
# ×©×™× ×•×™ ××¤×•×¨×˜ ×‘×¨×™×¨×ª ×”××—×“×œ 11434
export OLLAMA_HOST=0.0.0.0:8080

# ×”×¤×¢×œ×” ××—×“×© ×©×œ ollama
# Linux:
sudo systemctl restart ollama

# Mac/Windows:
# ×¡×’×•×¨ ×•×¤×ª×— ××—×“×© ××ª ×”××¤×œ×™×§×¦×™×”
```

---

## ğŸ“ ×©×™××•×© ×‘×¡×™×¡×™

### ×¤×§×•×“×•×ª CLI ×¢×™×§×¨×™×•×ª

#### 1. `ollama run` - ×”×¨×¦×” ××™× ×˜×¨××§×˜×™×‘×™×ª

```bash
# ×”×¨×¦×” ×‘×¡×™×¡×™×ª
ollama run llama3.1

# ×¢× prompt ××•×›×Ÿ ××¨××©
ollama run llama3.1 "×›×ª×•×‘ ×œ×™ ×¡×™×¤×•×¨ ×§×¦×¨ ×¢×œ ×—×ª×•×œ ×—×œ×œ"

# ×¢× ×¤×¨××˜×¨×™×
ollama run llama3.1 --verbose
```

#### 2. `ollama pull` - ×”×•×¨×“×ª ××•×“×œ

```bash
# ×”×•×¨×“×ª ××•×“×œ ×¡×¤×¦×™×¤×™
ollama pull deepseek-r1

# ×”×•×¨×“×ª ×’×¨×¡×” ×¡×¤×¦×™×¤×™×ª
ollama pull llama3.1:70b

# ×”×¦×’×ª ×”×ª×§×“××•×ª
ollama pull gemma3:27b --verbose
```

#### 3. `ollama list` - ×¨×©×™××ª ××•×“×œ×™×

```bash
# ×”×¦×’×ª ×›×œ ×”××•×“×œ×™×
ollama list

# ×¢× ××™×“×¢ ××¤×•×¨×˜
ollama list --verbose
```

#### 4. `ollama rm` - ××—×™×§×ª ××•×“×œ

```bash
# ××—×™×§×ª ××•×“×œ ×©××™× ×š ××©×ª××© ×‘×•
ollama rm llama2-uncensored

# ××—×™×§×ª ×’×¨×¡×” ×¡×¤×¦×™×¤×™×ª
ollama rm gemma3:27b
```

#### 5. `ollama ps` - ×ª×”×œ×™×›×™× ×¤×¢×™×œ×™×

```bash
# ×”×¦×’×ª ××•×“×œ×™× ×©×¨×¦×™× ×›×¨×’×¢
ollama ps
```

#### 6. `ollama cp` - ×”×¢×ª×§×ª ××•×“×œ

```bash
# ×™×¦×™×¨×ª ×¢×•×ª×§ ×¢× ×©× ××•×ª××
ollama cp llama3.1 my-custom-llama
```

### ×©×™××•×© ×‘-Multiline Prompts

```bash
ollama run gemma3 "
××ª×” ×¢×•×–×¨ ×ª×›× ×•×ª ××•××—×”.
×›×ª×•×‘ ×¤×•× ×§×¦×™×” ×‘-Python ×©××—×©×‘×ª ××ª ×”××¡×¤×¨ ×”×¤×™×‘×•× ××¦'×™ ×”-n.
×”×•×¡×£ ×”×¢×¨×•×ª ××¤×•×¨×˜×•×ª.
"
```

### ×©×™××•×© ×‘××¦×‘ Silent (×œ×œ× ×¤×œ×˜ ×‘×™× ×™×™×)

```bash
ollama run --verbose=false gemma3 "××” 2+2?"
```

---

## ğŸ”§ ×©×™××•×© ××ª×§×“×

### ×™×¦×™×¨×ª Modelfile ××•×ª×× ××™×©×™×ª

**Modelfile** ×”×•× ×§×•×‘×¥ ×ª×¦×•×¨×” ×©×××¤×©×¨ ×œ×”×ª××™× ××•×“×œ ×œ×¦×¨×›×™× ×©×œ×š.

#### ×“×•×’××” 1: ××•×“×œ ×¢× system prompt ××•×ª××

×¦×•×¨ ×§×•×‘×¥ ×‘×©× `Modelfile`:

```dockerfile
# ×©×™××•×© ×‘××•×“×œ ×‘×¡×™×¡
FROM llama3.1

# ×”×’×“×¨×ª ×”×”×ª× ×”×’×•×ª
SYSTEM """
××ª×” ×¢×•×–×¨ ×ª×›× ×•×ª ××•××—×” ×”××ª××—×” ×‘-Python ×•-JavaScript.
×ª××™×“ ×ª×¡×‘×™×¨ ××ª ×”×§×•×“ ×©×œ×š ×‘×¢×‘×¨×™×ª ×‘×¦×•×¨×” ×‘×¨×•×¨×” ×•×§×¦×¨×”.
×”×•×¡×£ ×“×•×’×××•×ª ×©×™××•×© ×œ×›×œ ×¤×•× ×§×¦×™×” ×©××ª×” ×›×•×ª×‘.
"""

# ×¤×¨××˜×¨×™×
PARAMETER temperature 0.7
PARAMETER top_p 0.9
PARAMETER top_k 40
```

×™×¦×™×¨×” ×•×”×¨×¦×”:

```bash
# ×™×¦×™×¨×ª ×”××•×“×œ ×”××•×ª××
ollama create code-assistant -f Modelfile

# ×”×¨×¦×ª ×”××•×“×œ
ollama run code-assistant
```

#### ×“×•×’××” 2: ××•×“×œ ××ª×•×¨×’×

```dockerfile
FROM mistral

SYSTEM """
××ª×” ××ª×¨×’× ××§×¦×•×¢×™.
×ª×¨×’× ×›×œ ×˜×§×¡×˜ ×©××ª×” ××§×‘×œ ××× ×’×œ×™×ª ×œ×¢×‘×¨×™×ª.
×©××•×¨ ×¢×œ ×”××©××¢×•×ª ×”××§×•×¨×™×ª ×•×”×˜×•×Ÿ.
"""

PARAMETER temperature 0.3
```

```bash
ollama create translator -f Modelfile
ollama run translator "Translate: Hello world"
```

#### ×“×•×’××” 3: ×™×™×‘×•× ××•×“×œ GGUF

```dockerfile
# ×™×™×‘×•× ××•×“×œ ××§×•××™ (GGUF)
FROM ./models/my-model.gguf

# ×”×•×¡×¤×ª ×”×•×¨××•×ª
SYSTEM "××ª×” ×¦'×˜×‘×•×˜ ×™×“×™×“×•×ª×™"

PARAMETER temperature 0.8
PARAMETER num_ctx 4096
```

```bash
ollama create my-imported-model -f Modelfile
```

### ×¤×¨××˜×¨×™× ×—×©×•×‘×™× ×‘-Modelfile

| ×¤×¨××˜×¨ | ×ª×™××•×¨ | ×¢×¨×›×™× | ×‘×¨×™×¨×ª ××—×“×œ |
|-------|-------|-------|------------|
| `temperature` | ×™×¦×™×¨×ª×™×•×ª (× ××•×š=×“×˜×¨××™× ×™×¡×˜×™, ×’×‘×•×”=×™×¦×™×¨×ª×™) | 0.0-2.0 | 0.8 |
| `top_p` | Nucleus sampling | 0.0-1.0 | 0.9 |
| `top_k` | ××¡×¤×¨ ×˜×•×§× ×™× ×œ×©×§×•×œ | 1-100 | 40 |
| `num_ctx` | ××•×¨×š ×”×”×§×©×¨ (context window) | 512-32768 | 2048 |
| `num_predict` | ××§×¡×™××•× ×˜×•×§× ×™× ×‘×ª×©×•×‘×” | -1-2048 | 128 |
| `repeat_penalty` | ×¢×•× ×© ×¢×œ ×—×–×¨×•×ª | 1.0-2.0 | 1.1 |

### ×©×™××•×© ×‘-API ×©×œ Ollama

Ollama ×—×•×©×¤×ª REST API ×¢×œ ×¤×•×¨×˜ 11434.

#### Python:

```python
import requests
import json

def chat_with_ollama(prompt, model="gemma3"):
    url = "http://localhost:11434/api/generate"
    
    payload = {
        "model": model,
        "prompt": prompt,
        "stream": False
    }
    
    response = requests.post(url, json=payload)
    
    if response.status_code == 200:
        return response.json()["response"]
    else:
        return f"×©×’×™××”: {response.status_code}"

# ×©×™××•×©
result = chat_with_ollama("××”×™ ×‘×™×¨×ª ×™×©×¨××œ?")
print(result)
```

#### ×¢× ×¡×¤×¨×™×™×ª ollama-python:

```bash
pip install ollama
```

```python
import ollama

# ×©×™×—×” ×‘×¡×™×¡×™×ª
response = ollama.chat(model='gemma3', messages=[
    {
        'role': 'user',
        'content': '×œ××” ×”×©××™× ×›×—×•×œ×™×?'
    }
])

print(response['message']['content'])

# ×©×™×—×” ×¢× streaming
for chunk in ollama.chat(
    model='gemma3',
    messages=[{'role': 'user', 'content': '×¡×¤×¨ ×œ×™ ×‘×“×™×—×”'}],
    stream=True
):
    print(chunk['message']['content'], end='', flush=True)
```

#### JavaScript/TypeScript:

```bash
npm install ollama
```

```javascript
import ollama from 'ollama';

// ×©×™×—×” ×‘×¡×™×¡×™×ª
const response = await ollama.chat({
    model: 'gemma3',
    messages: [{ role: 'user', content: '×©×œ×•×, ××” ×©×œ×•××š?' }]
});

console.log(response.message.content);

// ×¢× streaming
const stream = await ollama.chat({
    model: 'gemma3',
    messages: [{ role: 'user', content: '×›×ª×•×‘ ×¡×™×¤×•×¨ ×§×¦×¨' }],
    stream: true
});

for await (const chunk of stream) {
    process.stdout.write(chunk.message.content);
}
```

#### cURL (×œ×‘×“×™×§×•×ª):

```bash
# ×‘×§×©×” ×‘×¡×™×¡×™×ª
curl http://localhost:11434/api/generate -d '{
  "model": "gemma3",
  "prompt": "×œ××” ×”×©××™× ×›×—×•×œ×™×?",
  "stream": false
}'

# ×‘×“×™×§×ª ××•×“×œ×™× ×–××™× ×™×
curl http://localhost:11434/api/tags

# ××™×“×¢ ×¢×œ ××•×“×œ ×¡×¤×¦×™×¤×™
curl http://localhost:11434/api/show -d '{
  "name": "gemma3"
}'
```

---

## ğŸ’¡ ×˜×™×¤×™× ×•××•×¤×˜×™××™×–×¦×™×•×ª

### 1ï¸âƒ£ ×‘×—×™×¨×ª ×”××•×“×œ ×”× ×›×•×Ÿ

```bash
# ×œ××©×™××•×ª ×§×œ×•×ª ×•×—×¡×›×•×Ÿ ×‘××©××‘×™×:
ollama run gemma3:1b          # 815MB, ××”×™×¨ ×××•×“
ollama run llama3.2:1b        # 1.3GB, ×˜×•×‘ ×œ×¡×™×›×•××™×

# ×œ××™×–×•×Ÿ ×‘×™×Ÿ ××™×›×•×ª ×œ××”×™×¨×•×ª:
ollama run gemma3:4b          # 3.3GB, ×”××™×–×•×Ÿ ×”×˜×•×‘ ×‘×™×•×ª×¨
ollama run llama3.1           # 4.7GB, ××™×›×•×ª×™ ×•××”×™×¨

# ×œ××™×›×•×ª ××§×¡×™××œ×™×ª (×“×•×¨×© ××©××‘×™× ×¨×‘×™×):
ollama run llama3.3           # 43GB (70B ×¤×¨××˜×¨×™×)
ollama run deepseek-r1:671b   # 404GB (×œ××›×•× ×•×ª ×—×–×§×•×ª ×‘×œ×‘×“!)
```

### 2ï¸âƒ£ ××•×¤×˜×™××™×–×¦×™×” ×œ××”×™×¨×•×ª

#### ×”×’×“×œ×ª ××¡×¤×¨ ×ª×”×œ×™×›×™× ××§×‘×™×œ×™×:

```bash
# ×”×’×“×¨×ª ××¡×¤×¨ ×ª×”×œ×™×›×™× ×©×™×›×•×œ×™× ×œ×¨×•×¥ ×‘×•-×–×× ×™×ª
export OLLAMA_MAX_LOADED_MODELS=2

# ×”×’×“×¨×ª thread count
export OLLAMA_NUM_PARALLEL=4
```

#### ×©×™××•×© ×‘-GPU ×‘×¦×•×¨×” ×™×¢×™×œ×”:

```bash
# ×‘×“×™×§×” ×× GPU ××–×•×”×”
nvidia-smi  # ×¢×‘×•×¨ NVIDIA

# ×”×¨×¦×” ×¢× ×›×œ ×”×©×›×‘×•×ª ×‘-GPU
export OLLAMA_GPU_LAYERS=999  # ×˜×•×¢×Ÿ ××§×¡×™××•× ×©×›×‘×•×ª ××¤×©×¨×™

# ××• ×¨×§ ×—×œ×§ (×œ×—×™×¡×›×•×Ÿ ×‘-VRAM):
export OLLAMA_GPU_LAYERS=20
```

### 3ï¸âƒ£ × ×™×”×•×œ ×–×™×›×¨×•×Ÿ

```bash
# × ×™×§×•×™ ××•×“×œ×™× ×©×œ× ×‘×©×™××•×© ××”×–×™×›×¨×•×Ÿ
ollama stop <model-name>

# ×”×’×‘×œ×ª ×’×•×“×œ context (×—×•×¡×š ×–×™×›×¨×•×Ÿ):
ollama run gemma3 --num-ctx 2048

# ××—×™×§×ª ××•×“×œ×™× ×©×œ× ×¦×¨×™×š:
ollama rm <unused-model>
```

### 4ï¸âƒ£ ×©×™××•×© ×‘-Quantization

××•×“×œ×™× ××’×™×¢×™× ×‘×’×¨×¡××•×ª quantization ×©×•× ×•×ª:

```bash
# ×’×¨×¡××•×ª quantization × ×¤×•×¦×•×ª:
ollama pull llama3.1:q4_0     # 4-bit, ×§×˜×Ÿ ×××•×“ (×”×¤×—×ª×” ×‘××™×›×•×ª)
ollama pull llama3.1:q4_k_m   # 4-bit medium (××™×–×•×Ÿ ×˜×•×‘)
ollama pull llama3.1:q5_k_m   # 5-bit medium (××™×›×•×ª ×˜×•×‘×” ×™×•×ª×¨)
ollama pull llama3.1:q8_0     # 8-bit (××™×›×•×ª ×’×‘×•×”×”, ×™×•×ª×¨ ×›×‘×“)
ollama pull llama3.1:latest   # ×’×¨×¡×ª ×‘×¨×™×¨×ª ××—×“×œ (×‘×“×¨×š ×›×œ×œ q4)
```

ğŸ’¡ **×”××œ×¦×”**: `q4_k_m` × ×•×ª×Ÿ ××ª ×”××™×–×•×Ÿ ×”×˜×•×‘ ×‘×™×•×ª×¨ ×‘×™×Ÿ ×’×•×“×œ ×œ××™×›×•×ª.

### 5ï¸âƒ£ Batch Processing

```bash
# ×¢×™×‘×•×“ ××¨×•×‘×” prompts ××§×•×‘×¥
cat prompts.txt | while read prompt; do
    echo "=== $prompt ==="
    ollama run gemma3 "$prompt"
    echo ""
done
```

### 6ï¸âƒ£ ×©×™××•×© ×‘-RAG (Retrieval Augmented Generation)

×“×•×’××” ×‘×¡×™×¡×™×ª ×œ-RAG ×¢× Ollama:

```python
import ollama

# ××¡××›×™× ×œ×—×™×¤×•×© (×‘×¤×•×¢×œ ×™×”×™×• ×-vector DB)
documents = [
    "Ollama ×”×™× ×¤×œ×˜×¤×•×¨××” ×œ×”×¨×¦×ª LLMs ××§×•××™×ª",
    "×”×™× ×ª×•××›×ª ×‘××•×“×œ×™× ×›××• Llama, Gemma ×•-Mistral",
    "× ×™×ª×Ÿ ×œ×”×¨×™×¥ ××•×ª×” ×¢×œ Mac, Windows ×•-Linux"
]

def rag_query(question):
    # ×©×œ×‘ 1: ××¦×™××ª ××¡××›×™× ×¨×œ×•×•× ×˜×™×™× (×¤×©×˜× ×™ - ×‘×¤×•×¢×œ ×ª×©×ª××© ×‘-embeddings)
    context = "\n".join(documents)
    
    # ×©×œ×‘ 2: ×©××™×œ×ª×” ×¢× ×”×§×©×¨
    prompt = f"""
    ×‘×”×ª×‘×¡×¡ ×¢×œ ×”××™×“×¢ ×”×‘×:
    {context}
    
    ×©××œ×”: {question}
    
    ×ª×©×•×‘×”:
    """
    
    response = ollama.chat(model='gemma3', messages=[
        {'role': 'user', 'content': prompt}
    ])
    
    return response['message']['content']

# ×©×™××•×©
answer = rag_query("×¢×œ ××™×œ×• ××¢×¨×›×•×ª ×”×¤×¢×œ×” Ollama ×¤×•×¢×œ×ª?")
print(answer)
```

---

## âš ï¸ ×¤×ª×¨×•×Ÿ ×‘×¢×™×•×ª × ×¤×•×¦×•×ª

### ğŸ”´ ×‘×¢×™×”: "ollama: command not found"

**×¤×ª×¨×•×Ÿ Linux/Mac:**
```bash
# ×‘×“×•×§ ×× Ollama ××•×ª×§×Ÿ
which ollama

# ×× ×œ×, ×”×•×¡×£ ×œ-PATH
# Linux:
export PATH=$PATH:/usr/local/bin
echo 'export PATH=$PATH:/usr/local/bin' >> ~/.bashrc

# Mac:
export PATH="/Applications/Ollama.app/Contents/MacOS:$PATH"
echo 'export PATH="/Applications/Ollama.app/Contents/MacOS:$PATH"' >> ~/.zshrc
```

**×¤×ª×¨×•×Ÿ Windows:**
```powershell
# ×‘×“×•×§ ×× ×‘×ª×™×§×™×™×ª ×”×ª×§× ×”
cd $env:LOCALAPPDATA\Programs\Ollama
.\ollama.exe --version

# ×× ×¢×•×‘×“, ×”×•×¡×£ ×œ-PATH
$env:Path += ";$env:LOCALAPPDATA\Programs\Ollama"
```

---

### ğŸ”´ ×‘×¢×™×”: "Failed to load model"

**×¡×™×‘×•×ª ××¤×©×¨×™×•×ª:**

1. **××™×Ÿ ××¡×¤×™×§ RAM:**
```bash
# ×‘×“×•×§ ×©×™××•×© ×‘×–×™×›×¨×•×Ÿ
free -h  # Linux
vm_stat  # Mac

# ×”×©×ª××© ×‘××•×“×œ ×§×˜×Ÿ ×™×•×ª×¨
ollama run gemma3:1b
```

2. **×”××•×“×œ ×œ× ×”×•×¨×“ ×‘××œ×•××•:**
```bash
# ××—×§ ×•×”×•×¨×“ ××—×“×©
ollama rm gemma3
ollama pull gemma3
```

3. **×§×‘×¦×™× ×¤×’×•××™×:**
```bash
# × ×§×” cache ×•× ×¡×” ×©×•×‘
rm -rf ~/.ollama/models/<model-name>
ollama pull <model-name>
```

---

### ğŸ”´ ×‘×¢×™×”: ×”×“×•×¨ ××™×˜×™ ×××•×“

**××‘×—×•×Ÿ:**

```bash
# ×‘×“×•×§ ×× GPU ××–×•×”×”
nvidia-smi  # NVIDIA
rocm-smi    # AMD

# ×‘×“×•×§ ××™×–×” device ×‘×©×™××•×©
ollama ps
```

**×¤×ª×¨×•×Ÿ:**

```bash
# ××™×œ×•×¥ ×©×™××•×© ×‘-CPU (×× GPU ×’×•×¨× ×œ×‘×¢×™×•×ª)
export OLLAMA_COMPUTE=cpu
sudo systemctl restart ollama

# ××• ×”×’×‘×œ ×©×›×‘×•×ª ×‘-

---

## ğŸ“Š ×¡×˜×˜×™×¡×˜×™×§×•×ª ×”×¤×¨×•×™×§×˜

- **×›×•×›×‘×™×**: 156,999 â­
- **Forks**: 13,818 ğŸ”±
- **Issues**: 2,315 ğŸ›
- **×©×¤×”**: Go ğŸ’»
- **×¨×™×©×™×•×Ÿ**: MIT License ğŸ“œ

## ğŸ”— ×§×™×©×•×¨×™× ×©×™××•×©×™×™×

- [×¨×™×¤×• ×‘-GitHub](https://github.com/ollama/ollama)
- [Issues & ×ª××™×›×”](https://github.com/ollama/ollama/issues)
- [Discussions](https://github.com/ollama/ollama/discussions)
- [Wiki](https://github.com/ollama/ollama/wiki)

---

*××“×¨×™×š ×–×” × ×•×¦×¨ ××•×˜×•××˜×™×ª ×¢×œ ×™×“×™ AI Guide Bot ×¢× Claude AI*
*×¢×“×›×•×Ÿ ××—×¨×•×Ÿ: 03/12/2025 19:13*
