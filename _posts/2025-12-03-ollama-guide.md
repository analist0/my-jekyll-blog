---
layout: post
title: "××“×¨×™×š ××§×¦×•×¢×™: ollama - Get up and running with OpenAI gpt-oss, DeepSeek-R1, Gemma 3 and other models."
date: 2025-12-03 18:58:45 +0200
categories: [AI, LLM, ××“×¨×™×›×™×]
tags: [local-ai, llm, installation, go]
image: /assets/images/repos/ollama-20251203.png
author: AI Guide Bot
lang: he
dir: rtl
---

![ollama](/assets/images/repos/ollama-20251203.png)

# ğŸš€ ollama

**â­ 156,999 ×›×•×›×‘×™× | ğŸ”§ Go | ğŸ“… ×¢×“×›×•×Ÿ ××—×¨×•×Ÿ: 2025-12-03**

[ğŸ”— ×§×™×©×•×¨ ×œ×¨×™×¤×•](https://github.com/ollama/ollama) | [â¬‡ï¸ ×”×•×¨×“×”](https://github.com/ollama/ollama/archive/refs/heads/main.zip)

---

# ğŸ¦™ ××“×¨×™×š ×”×ª×§× ×” ××§×™×£ ×œ-Ollama - ×”×¨×¦×ª ××•×“×œ×™ AI ××§×•××™×ª

## ğŸ“‹ ×ª×•×›×Ÿ ×¢× ×™×™× ×™×
1. [×¡×§×™×¨×” ×›×œ×œ×™×ª](#×¡×§×™×¨×”-×›×œ×œ×™×ª)
2. [×“×¨×™×©×•×ª ××¢×¨×›×ª](#×“×¨×™×©×•×ª-××¢×¨×›×ª)
3. [×”×ª×§× ×” ×¦×¢×“ ××—×¨ ×¦×¢×“](#×”×ª×§× ×”-×¦×¢×“-××—×¨-×¦×¢×“)
4. [×”×’×“×¨×” ×¨××©×•× ×™×ª](#×”×’×“×¨×”-×¨××©×•× ×™×ª)
5. [×©×™××•×© ×‘×¡×™×¡×™](#×©×™××•×©-×‘×¡×™×¡×™)
6. [×˜×™×¤×™× ××ª×§×“××™×](#×˜×™×¤×™×-××ª×§×“××™×)
7. [×¤×ª×¨×•×Ÿ ×‘×¢×™×•×ª × ×¤×•×¦×•×ª](#×¤×ª×¨×•×Ÿ-×‘×¢×™×•×ª-× ×¤×•×¦×•×ª)
8. [××©××‘×™× × ×•×¡×¤×™×](#××©××‘×™×-× ×•×¡×¤×™×)

---

## ğŸ¯ ×¡×§×™×¨×” ×›×œ×œ×™×ª

### ××” ×–×” Ollama?

**Ollama** ×”×™× ×¤×œ×˜×¤×•×¨××” ××§×•××™×ª ×œ×”×¨×¦×ª ××•×“×œ×™ ×©×¤×” ×’×“×•×œ×™× (LLM) ×¢×œ ×”××—×©×‘ ×”××™×©×™ ×©×œ×š. ×¢× ×œ××¢×œ×” ×-**156,000 ×›×•×›×‘×™×** ×‘-GitHub, ×–×”×• ×”×›×œ×™ ×”××•×‘×™×œ ×œ×”×¨×¦×ª AI ×‘××•×¤×Ÿ ×¤×¨×˜×™ ×•××§×•××™.

### ğŸŒŸ ×™×ª×¨×•× ×•×ª ××¨×›×–×™×™×

- **ğŸ”’ ×¤×¨×˜×™×•×ª ××œ××”** - ×›×œ ×”× ×ª×•× ×™× × ×©××¨×™× ×‘××—×©×‘ ×©×œ×š
- **âš¡ ×‘×™×¦×•×¢×™× ××”×™×¨×™×** - ××™×Ÿ ×ª×œ×•×ª ×‘×—×™×‘×•×¨ ××™× ×˜×¨× ×˜
- **ğŸ’° ×—×™× ××™ ×œ×—×œ×•×˜×™×Ÿ** - ×œ×œ× ×¢×œ×•×™×•×ª API
- **ğŸ¨ ×”×ª×××” ××™×©×™×ª** - ×™×›×•×œ×ª ×œ×”×ª××™× ××•×“×œ×™× ×œ×¦×¨×›×™× ×©×œ×š
- **ğŸ”Œ ××™× ×˜×’×¨×¦×™×” ×¤×©×•×˜×”** - ×¡×¤×¨×™×•×ª Python ×•-JavaScript

### ×œ××™ ×–×” ××™×•×¢×“?

âœ… ××¤×ª×—×™× ×©×¨×•×¦×™× ×œ×©×œ×‘ AI ×‘××¤×œ×™×§×¦×™×•×ª  
âœ… ×—×•×§×¨×™× ×©×–×§×•×§×™× ×œ×¤×¨×˜×™×•×ª ××œ××”  
âœ… ×—×•×‘×‘×™ AI ×©×¨×•×¦×™× ×œ×”×ª× ×¡×•×ª ×‘××•×“×œ×™× ×©×•× ×™×  
âœ… ××¨×’×•× ×™× ×¢× ×“×¨×™×©×•×ª ××‘×˜×—×ª ××™×“×¢ ××—××™×¨×•×ª

---

## ğŸ’» ×“×¨×™×©×•×ª ××¢×¨×›×ª

### ×“×¨×™×©×•×ª ×—×•××¨×” ××™× ×™××œ×™×•×ª

#### ××•×“×œ×™× ×§×˜× ×™× (1B-7B ×¤×¨××˜×¨×™×)
- **RAM**: 8GB ××™× ×™××•×
- **××—×¡×•×Ÿ**: 10GB ×¤× ×•×™×™×
- **××¢×‘×“**: Intel i5/AMD Ryzen 5 ××• ×˜×•×‘ ×™×•×ª×¨
- **GPU** (××•×¤×¦×™×•× ×œ×™): NVIDIA ×¢× 4GB VRAM

#### ××•×“×œ×™× ×‘×™× ×•× ×™×™× (13B-33B ×¤×¨××˜×¨×™×)
- **RAM**: 16GB ××™× ×™××•×
- **××—×¡×•×Ÿ**: 30GB ×¤× ×•×™×™×
- **××¢×‘×“**: Intel i7/AMD Ryzen 7
- **GPU** (××•××œ×¥): NVIDIA ×¢× 8GB VRAM

#### ××•×“×œ×™× ×’×“×•×œ×™× (70B+ ×¤×¨××˜×¨×™×)
- **RAM**: 32GB ×•××¢×œ×”
- **××—×¡×•×Ÿ**: 100GB+ ×¤× ×•×™×™×
- **××¢×‘×“**: Intel i9/AMD Ryzen 9
- **GPU** (×”×›×¨×—×™): NVIDIA RTX 3090/4090 ××• A100

### âš ï¸ ×”×¢×¨×•×ª ×—×©×•×‘×•×ª

> **×©×™××• ×œ×‘**: ×”×¨×¦×ª ××•×“×œ×™× ×¢×œ CPU ×‘×œ×‘×“ ××¤×©×¨×™×ª ××š ×ª×”×™×” **××™×˜×™×ª ××©××¢×•×ª×™×ª**. ××•××œ×¥ ×××•×“ GPU ×©×œ NVIDIA ×¢× ×ª××™×›×ª CUDA.

### ××¢×¨×›×•×ª ×”×¤×¢×œ×” × ×ª××›×•×ª

- ğŸ **macOS** 11 Big Sur ×•××¢×œ×” (Intel ×•-Apple Silicon)
- ğŸªŸ **Windows** 10/11 (64-bit)
- ğŸ§ **Linux** - ×¨×•×‘ ×”×”×¤×¦×•×ª (Ubuntu, Debian, Fedora, Arch ×•×›×•')
- ğŸ³ **Docker** - ×›×œ ×¤×œ×˜×¤×•×¨××” ×¢× Docker

---

## ğŸš€ ×”×ª×§× ×” ×¦×¢×“ ××—×¨ ×¦×¢×“

### ğŸ macOS

#### ×©×™×˜×” 1: ×”×ª×§× ×” ×’×¨×¤×™×ª (××•××œ×¥ ×œ××ª×—×™×œ×™×)

1. **×”×•×¨×“×ª ×”×§×•×‘×¥**
   ```bash
   # ×’×©×• ×œ×“×¤×“×¤×Ÿ ×•×”×•×¨×™×“×• ××ª ×”×§×•×‘×¥
   # ××• ×”×©×ª××©×• ×‘-curl:
   curl -L https://ollama.com/download/Ollama.dmg -o Ollama.dmg
   ```

2. **×”×ª×§× ×”**
   - ×¤×ª×—×• ××ª ×”×§×•×‘×¥ `Ollama.dmg`
   - ×’×¨×¨×• ××ª Ollama ×œ×ª×™×§×™×™×ª Applications
   - ×¤×ª×—×• ××ª Ollama ××”-Launchpad

3. **××™××•×ª ×”×”×ª×§× ×”**
   ```bash
   # ×¤×ª×—×• Terminal ×•×‘×“×§×•:
   ollama --version
   ```

#### ×©×™×˜×” 2: ×”×ª×§× ×” ×“×¨×š Homebrew

```bash
# ×× ××™×Ÿ ×œ×›× Homebrew, ×”×ª×§×™× ×• ××•×ª×• ×ª×—×™×œ×”:
/bin/bash -c "$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)"

# ×”×ª×§× ×ª Ollama
brew install ollama

# ×”×¤×¢×œ×ª ×”×©×™×¨×•×ª
brew services start ollama
```

### ğŸªŸ Windows

#### ×”×ª×§× ×” ×¡×˜× ×“×¨×˜×™×ª

1. **×”×•×¨×“×” ×•×”×ª×§× ×”**
   ```powershell
   # ×”×•×¨×™×“×• ××ª ×”×§×•×‘×¥ ××• ×”×©×ª××©×• ×‘-PowerShell:
   Invoke-WebRequest -Uri "https://ollama.com/download/OllamaSetup.exe" -OutFile "OllamaSetup.exe"
   
   # ×”×¨×¦×ª ×”×”×ª×§× ×”
   .\OllamaSetup.exe
   ```

2. **×¢×§×‘×• ××—×¨ ××©×£ ×”×”×ª×§× ×”**
   - ×œ×—×¦×• Next/Install
   - ×‘×—×¨×• ××ª ×ª×™×§×™×™×ª ×”×”×ª×§× ×” (×‘×¨×™×¨×ª ××—×“×œ: `C:\Program Files\Ollama`)
   - ×”××ª×™× ×• ×œ×¡×™×•× ×”×”×ª×§× ×”

3. **××™××•×ª**
   ```powershell
   # ×¤×ª×—×• PowerShell ××• CMD
   ollama --version
   ```

#### âš™ï¸ ×”×’×“×¨×ª GPU ×‘-Windows

```powershell
# ×‘×“×™×§×ª ×ª××™×›×ª CUDA (×œ×‘×¢×œ×™ ×›×¨×˜×™×¡×™ NVIDIA)
nvidia-smi

# ×× ×”×¤×§×•×“×” ×¢×•×‘×“×ª, Ollama ×™×–×”×” ××•×˜×•××˜×™×ª ××ª ×”-GPU
# ××—×¨×ª, ×”×ª×§×™× ×• ××ª NVIDIA CUDA Toolkit:
# https://developer.nvidia.com/cuda-downloads
```

### ğŸ§ Linux

#### ×©×™×˜×” 1: ×¡×§×¨×™×¤×˜ ×”×ª×§× ×” ××•×˜×•××˜×™ (××•××œ×¥)

```bash
# ×”×ª×§× ×” ×‘×¤×§×•×“×” ××—×ª
curl -fsSL https://ollama.com/install.sh | sh

# ×”×¤×¢×œ×ª ×”×©×™×¨×•×ª
sudo systemctl start ollama
sudo systemctl enable ollama  # ×”×¤×¢×œ×” ××•×˜×•××˜×™×ª ×‘×¢×ª ××ª×—×•×œ
```

#### ×©×™×˜×” 2: ×”×ª×§× ×” ×™×“× ×™×ª (×œ××ª×§×“××™×)

```bash
# 1. ×”×•×¨×“×ª ×”×§×•×‘×¥ ×”×‘×™× ××¨×™
curl -L https://ollama.com/download/ollama-linux-amd64 -o ollama
chmod +x ollama
sudo mv ollama /usr/local/bin/

# 2. ×™×¦×™×¨×ª ××©×ª××© ××¢×¨×›×ª
sudo useradd -r -s /bin/false -m -d /usr/share/ollama ollama

# 3. ×™×¦×™×¨×ª ×§×•×‘×¥ systemd service
sudo tee /etc/systemd/system/ollama.service > /dev/null <<EOF
[Unit]
Description=Ollama Service
After=network-online.target

[Service]
ExecStart=/usr/local/bin/ollama serve
User=ollama
Group=ollama
Restart=always
RestartSec=3
Environment="PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin"

[Install]
WantedBy=default.target
EOF

# 4. ×”×¤×¢×œ×ª ×”×©×™×¨×•×ª
sudo systemctl daemon-reload
sudo systemctl enable ollama
sudo systemctl start ollama
```

#### ğŸ® ×”×’×“×¨×ª GPU ×‘-Linux (NVIDIA)

```bash
# ×”×ª×§× ×ª NVIDIA Container Toolkit (×× ××©×ª××©×™× ×‘-Docker)
distribution=$(. /etc/os-release;echo $ID$VERSION_ID)
curl -s -L https://nvidia.github.io/libnvidia-container/gpgkey | sudo apt-key add -
curl -s -L https://nvidia.github.io/libnvidia-container/$distribution/libnvidia-container.list | \
  sudo tee /etc/apt/sources.list.d/nvidia-container-toolkit.list

sudo apt-get update
sudo apt-get install -y nvidia-container-toolkit

# ××™××•×ª
nvidia-smi
```

#### ğŸ¯ Ubuntu/Debian - ×”×ª×§× ×” ××”×™×¨×”

```bash
# ×¢×“×›×•×Ÿ ×”××¢×¨×›×ª
sudo apt update && sudo apt upgrade -y

# ×”×ª×§× ×ª ×ª×œ×•×™×•×ª
sudo apt install -y curl

# ×”×ª×§× ×ª Ollama
curl -fsSL https://ollama.com/install.sh | sh

# ×‘×“×™×§×ª ×¡×˜×˜×•×¡
sudo systemctl status ollama
```

#### ğŸ”´ Fedora/RHEL - ×”×ª×§× ×” ××”×™×¨×”

```bash
# ×¢×“×›×•×Ÿ ×”××¢×¨×›×ª
sudo dnf update -y

# ×”×ª×§× ×ª Ollama
curl -fsSL https://ollama.com/install.sh | sh

# ×‘×“×™×§×ª ×¡×˜×˜×•×¡
sudo systemctl status ollama
```

### ğŸ³ Docker

#### ×©×™×˜×” 1: Docker ×¨×’×™×œ (CPU ×‘×œ×‘×“)

```bash
# ×”×¨×¦×ª Ollama ×›×§×•× ×˜×™×™× ×¨
docker run -d \
  -v ollama:/root/.ollama \
  -p 11434:11434 \
  --name ollama \
  ollama/ollama

# ××™××•×ª
docker ps | grep ollama
```

#### ×©×™×˜×” 2: Docker ×¢× GPU (NVIDIA)

```bash
# ×”×¨×¦×” ×¢× ×ª××™×›×ª GPU
docker run -d \
  --gpus=all \
  -v ollama:/root/.ollama \
  -p 11434:11434 \
  --name ollama \
  ollama/ollama

# ×‘×“×™×§×ª ×’×™×©×” ×œ-GPU
docker exec ollama nvidia-smi
```

#### ğŸ“¦ Docker Compose

×¦×¨×• ×§×•×‘×¥ `docker-compose.yml`:

```yaml
version: '3.8'

services:
  ollama:
    image: ollama/ollama:latest
    container_name: ollama
    ports:
      - "11434:11434"
    volumes:
      - ollama_data:/root/.ollama
    # ×”×¡×™×¨×• ××ª ×”×”×¢×¨×” ×œ××—×©×‘×™× ×¢× GPU
    # deploy:
    #   resources:
    #     reservations:
    #       devices:
    #         - driver: nvidia
    #           count: all
    #           capabilities: [gpu]
    restart: unless-stopped

volumes:
  ollama_data:
```

```bash
# ×”×¤×¢×œ×”
docker-compose up -d

# ×‘×“×™×§×ª ×œ×•×’×™×
docker-compose logs -f ollama
```

### ğŸ“± Termux (Android)

> âš ï¸ **×©×™××• ×œ×‘**: ×”×¨×¦×ª Ollama ×¢×œ ×× ×“×¨×•××™×“ ×“×•×¨×©×ª ××›×©×™×¨ ×—×–×§ (8GB+ RAM) ×•×”×™× **× ×™×¡×™×•× ×™×ª**.

```bash
# 1. ×”×ª×§× ×ª Termux ×-F-Droid (×œ× ×-Play Store!)
# https://f-droid.org/en/packages/com.termux/

# 2. ×¢×“×›×•×Ÿ ×—×‘×™×œ×•×ª
pkg update && pkg upgrade -y

# 3. ×”×ª×§× ×ª ×ª×œ×•×™×•×ª
pkg install -y proot-distro

# 4. ×”×ª×§× ×ª Ubuntu ×‘×ª×•×š Termux
proot-distro install ubuntu
proot-distro login ubuntu

# 5. ×›×¢×ª ×‘×ª×•×š Ubuntu, ×”×¨×™×¦×•:
apt update && apt install -y curl
curl -fsSL https://ollama.com/install.sh | sh

# 6. ×”×¤×¢×œ×ª Ollama
ollama serve &
```

---

## âš™ï¸ ×”×’×“×¨×” ×¨××©×•× ×™×ª

### ×‘×“×™×§×ª ×”×ª×§× ×” ×ª×§×™× ×”

```bash
# ×‘×“×™×§×ª ×’×¨×¡×”
ollama --version

# ×‘×“×™×§×ª ×—×™×‘×•×¨ ×œ×©×¨×ª
curl http://localhost:11434/api/version
```

**×¤×œ×˜ ×¦×¤×•×™:**
```json
{"version":"0.1.XX"}
```

### ×”×•×¨×“×ª ×”××•×“×œ ×”×¨××©×•×Ÿ

```bash
# ×”×•×¨×“×ª Gemma 3 (××•×“×œ ×§×˜×Ÿ ×•×˜×•×‘ ×œ××ª×—×™×œ×™× - 815MB)
ollama pull gemma3:1b

# ×”×•×¨×“×ª DeepSeek-R1 (××•×“×œ ××¦×•×™×Ÿ ×œ×—×©×™×‘×” - 4.7GB)
ollama pull deepseek-r1

# ×”×•×¨×“×ª Llama 3.2 (××•×“×œ ×××•×–×Ÿ - 2GB)
ollama pull llama3.2
```

### ğŸ“Š ×”×¦×’×ª ××•×“×œ×™× ××•×ª×§× ×™×

```bash
# ×¨×©×™××ª ×›×œ ×”××•×“×œ×™× ×‘××—×©×‘
ollama list
```

**×¤×œ×˜ ×œ×“×•×’××”:**
```
NAME              ID              SIZE    MODIFIED
gemma3:1b        abc123def456     815 MB  2 hours ago
deepseek-r1      789ghi012jkl     4.7 GB  5 minutes ago
llama3.2         mno345pqr678     2.0 GB  1 day ago
```

### ğŸ—‘ï¸ ××—×™×§×ª ××•×“×œ×™×

```bash
# ××—×™×§×ª ××•×“×œ ×¡×¤×¦×™×¤×™
ollama rm gemma3:1b

# ××—×™×§×ª ×›×œ ×”××•×“×œ×™× (×–×”×™×¨×•×ª!)
ollama list | awk 'NR>1 {print $1}' | xargs -I {} ollama rm {}
```

### ğŸ”§ ×”×’×“×¨×•×ª ×¡×‘×™×‘×” ××ª×§×“××•×ª

#### ×©×™× ×•×™ ×ª×™×§×™×™×ª ××—×¡×•×Ÿ ×”××•×“×œ×™×

**Linux/Mac:**
```bash
# ×”×•×¡×¤×” ×œ-~/.bashrc ××• ~/.zshrc
export OLLAMA_MODELS="/path/to/your/models"

# ×˜×¢×™× ×” ××—×“×©
source ~/.bashrc
```

**Windows (PowerShell):**
```powershell
# ×”×•×¡×¤×” ×œ××©×ª× ×” ×¡×‘×™×‘×” ×§×‘×•×¢
[System.Environment]::SetEnvironmentVariable('OLLAMA_MODELS', 'D:\OllamaModels', 'User')
```

#### ×”×’×“×¨×ª ××¡×¤×¨ ×”×œ×™×›×™× (threads)

```bash
# ×”×’×“×¨×ª 8 ×œ×™×‘×•×ª
export OLLAMA_NUM_THREADS=8

# ×”×¤×¢×œ×ª Ollama ×¢× ×”×”×’×“×¨×”
ollama serve
```

#### ×©×™× ×•×™ ×¤×•×¨×˜ ×‘×¨×™×¨×ª ×”××—×“×œ

```bash
# ×©×™××•×© ×‘×¤×•×¨×˜ 8080 ×‘××§×•× 11434
export OLLAMA_HOST=0.0.0.0:8080
ollama serve
```

---

## ğŸ® ×©×™××•×© ×‘×¡×™×¡×™

### ×¦'××˜ ××™× ×˜×¨××§×˜×™×‘×™

```bash
# ×¤×ª×™×—×ª ×¦'××˜ ×¢× Gemma 3
ollama run gemma3

# ×“×•×’××ª ×©×™×—×”:
>>> ×”×™×™, ××” ×”××–×’ ×”××•×•×™×¨ ×”×™×•×?
×× ×™ ××•×“×œ AI ×•××™×Ÿ ×œ×™ ×’×™×©×” ×œ××™×“×¢ ×‘×–××Ÿ ×××ª ×›×•×œ×œ ××–×’ ××•×•×™×¨. 
×× ×™ ×××œ×™×¥ ×œ×‘×“×•×§ ×‘××¤×œ×™×§×¦×™×™×ª ××–×’ ××•×•×™×¨ ××• ×‘××ª×¨ ×™×™×¢×•×“×™.

>>> ×ª×•×“×”! /bye
```

#### ×¤×§×•×“×•×ª ×©×™××•×©×™×•×ª ×‘×¦'××˜

- `/bye` - ×™×¦×™××” ××”×¦'××˜
- `/clear` - × ×™×§×•×™ ×”×”×™×¡×˜×•×¨×™×”
- `/show info` - ×”×¦×’×ª ××™×“×¢ ×¢×œ ×”××•×“×œ
- `/show modelfile` - ×”×¦×’×ª ×ª×¦×•×¨×ª ×”××•×“×œ
- `/?` - ×¢×–×¨×”

### ×”×¨×¦×ª ×¤×§×•×“×” ×‘×•×“×“×ª

```bash
# ×©××œ×” ×‘×•×“×“×ª ×œ×œ× ×¦'××˜ ××™× ×˜×¨××§×˜×™×‘×™
ollama run gemma3 "××”×™ ×‘×™× ×” ××œ××›×•×ª×™×ª?"

# ×¢× ×”×¤× ×™×™×ª ×¤×œ×˜ ×œ×§×•×‘×¥
ollama run llama3.2 "×›×ª×•×‘ ×©×™×¨ ×¢×œ ×”×—×™×™×" > poem.txt
```

### ğŸ ×©×™××•×© ×‘-Python

#### ×”×ª×§× ×ª ×”×¡×¤×¨×™×™×”

```bash
pip install ollama
```

#### ×“×•×’××” ×‘×¡×™×¡×™×ª

```python
import ollama

# ×©×™×—×” ×¤×©×•×˜×”
response = ollama.chat(model='gemma3', messages=[
    {
        'role': 'user',
        'content': '×œ××” ×”×©××™×™× ×›×—×•×œ×™×?',
    },
])

print(response['message']['content'])
```

#### ×“×•×’××” ××ª×§×“××ª ×¢× streaming

```python
import ollama

# ×”×¦×’×ª ×ª×©×•×‘×” ×‘×–××Ÿ ×××ª
stream = ollama.chat(
    model='deepseek-r1',
    messages=[{'role': 'user', 'content': '×”×¡×‘×¨ ×¢×œ ×—×•×¨×™× ×©×—×•×¨×™×'}],
    stream=True,
)

for chunk in stream:
    print(chunk['message']['content'], end='', flush=True)
```

#### ×™×¦×™×¨×ª chatbot ×¢× ×”×§×©×¨

```python
import ollama

conversation_history = []

def chat(user_message):
    # ×”×•×¡×¤×ª ×”×•×“×¢×ª ×”××©×ª××© ×œ×”×™×¡×˜×•×¨×™×”
    conversation_history.append({
        'role': 'user',
        'content': user_message
    })
    
    # ×§×‘×œ×ª ×ª×©×•×‘×”
    response = ollama.chat(
        model='llama3.2',
        messages=conversation_history
    )
    
    # ×”×•×¡×¤×ª ×ª×©×•×‘×ª ×”××•×“×œ ×œ×”×™×¡×˜×•×¨×™×”
    conversation_history.append({
        'role': 'assistant',
        'content': response['message']['content']
    })
    
    return response['message']['content']

# ×©×™××•×©
print(chat("×”×™×™, ×§×•×¨××™× ×œ×™ ×“× ×™"))
print(chat("××™×š ×§×•×¨××™× ×œ×™?"))  # ×”××•×“×œ ×™×–×›×•×¨!
```

### ğŸ’» ×©×™××•×© ×‘-JavaScript/TypeScript

#### ×”×ª×§× ×ª ×”×¡×¤×¨×™×™×”

```bash
npm install ollama
# ××•
yarn add ollama
```

#### ×“×•×’××” ×‘×¡×™×¡×™×ª

```javascript
import ollama from 'ollama';

async function main() {
    const response = await ollama.chat({
        model: 'gemma3',
        messages: [{ role: 'user', content: '××”×™ ×ª×›× ×•×ª?' }],
    });
    
    console.log(response.message.content);
}

main();
```

#### ×“×•×’××” ×¢× streaming

```javascript
import ollama from 'ollama';

async function streamChat() {
    const stream = await ollama.chat({
        model: 'llama3.2',
        messages: [{ role: 'user', content: '×¡×¤×¨ ×œ×™ ×¡×™×¤×•×¨ ×§×¦×¨' }],
        stream: true,
    });

    for await (const chunk of stream) {
        process.stdout.write(chunk.message.content);
    }
}

streamChat();
```

### ğŸŒ ×©×™××•×© ×‘-REST API

#### ×‘×“×™×§×ª ×–××™× ×•×ª

```bash
curl http://localhost:11434/api/version
```

#### ×©×œ×™×—×ª ×©××œ×”

```bash
curl -X POST http://localhost:11434/api/generate -d '{
  "model": "gemma3",
  "prompt": "××“×•×¢ ×”×™×¨×— ×–×•×¨×— ×‘×œ×™×œ×”?",
  "stream": false
}'
```

#### ×¦'××˜ ×¢× ×”×§×©×¨

```bash
curl -X POST http://localhost:11434/api/chat -d '{
  "model": "llama3.2",
  "messages": [
    {
      "role": "system",
      "content": "××ª×” ×¢×•×–×¨ ××•×¢×™×œ ×©××“×‘×¨ ×‘×¢×‘×¨×™×ª"
    },
    {
      "role": "user",
      "content": "××”×• ×¤×™×ª×•×Ÿ?"
    }
  ],
  "stream": false
}'
```

---

## ğŸš€ ×˜×™×¤×™× ××ª×§×“××™×

### 1. ×™×¦×™×¨×ª Modelfile ××•×ª×× ××™×©×™×ª

#### ×“×•×’××”: ×‘×•×˜ ×ª××™×›×” ×˜×›× ×™×ª

×¦×¨×• ×§×•×‘×¥ ×‘×©× `Modelfile`:

```dockerfile
FROM llama3.2

# ×”×’×“×¨×ª ×¤×¨××˜×¨×™ ×˜××¤×¨×˜×•×¨×” (×™×¦×™×¨×ª×™×•×ª)
PARAMETER temperature 0.7
PARAMETER top_p 0.9

# ×”×’×“×¨×ª ×”×§×©×¨ ××¢×¨×›×ª
SYSTEM """
××ª×” ××•××—×” ×ª××™×›×” ×˜×›× ×™×ª ×¢× × ×™×¡×™×•×Ÿ ×©×œ 10 ×©× ×™×.
××ª×” ××“×‘×¨ ×‘×¢×‘×¨×™×ª ×‘×¦×•×¨×” ×‘×¨×•×¨×” ×•××§×¦×•×¢×™×ª.
××ª×” ×ª××™×“ ××¦×™×¢ ×¤×ª×¨×•× ×•×ª ××¢×©×™×™× ×•×‘×“×•×§×™×.
"""

# ×”×’×“×¨×ª ×ª×‘× ×™×ª
TEMPLATE """
{{ .System }}

×©××œ×”: {{ .Prompt }}

×ª×©×•×‘×” ××¤×•×¨×˜×ª:
"""
```

#### ×™×¦×™×¨×” ×•×”×¨×¦×”

```bash
# ×™×¦×™×¨×ª ×”××•×“×œ ×”××•×ª××
ollama create tech-support -f ./Modelfile

# ×”×¨×¦×”
ollama run tech-support "×”××—×©×‘ ×©×œ×™ ×œ× × ×“×œ×§"
```

### 2. ×©×™××•×© ×‘××•×“×œ×™ ×¨××™×™×” (Vision Models)

```bash
# ×”×•×¨×“×ª ××•×“×œ ×¨××™×™×”
ollama pull llama3.2-vision

# × ×™×ª×•×— ×ª××•× ×”
ollama run llama3.2-vision "×ª××¨ ××ª ×”×ª××•× ×” ×”×–×•" < /path/to/image.jpg
```

#### ×“×•×’××” ×‘-Python

```python
import ollama

# × ×™×ª×•×— ×ª××•× ×”
with open('photo.jpg', 'rb') as file:
    response = ollama.chat(
        model='llama3.2-vision',
        messages=[{
            'role': 'user',
            'content': '××” ××ª×” ×¨×•××” ×‘×ª××•× ×”?',
            'images': [file.read()]
        }]
    )
    
print(response['message']['content'])
```

### 3. ××•×¤×˜×™××™×–×¦×™×” ×œ×‘×™×¦×•×¢×™×

#### ×”×’×“×¨×ª quantization (×“×—×™×¡×”)

```bash
# ×”×•×¨×“×ª ×’×¨×¡×” ×“×—×•×¡×” ×™×•×ª×¨ (××”×™×¨×” ×™×•×ª×¨, ×¤×—×•×ª ××“×•×™×§×ª)
ollama pull llama3.2:Q4_K_M  # ×“×—×™×¡×” ×‘×™× ×•× ×™×ª
ollama pull llama3.2:Q2_K    # ×“×—×™×¡×” ×’×‘×•×”×” (×”×›×™ ××”×™×¨)
```

#### ×©×™××•×© ×‘-GPU layers

```bash
# ×”×’×“×¨×ª ××¡×¤×¨ layers ×‘-GPU (Linux/Mac)
export OLLAMA_NUM_GPU=32  # ×›×œ ×”-layers
ollama run llama3.2
```

#### ×”×’×“×¨×ª context window

```bash
# ×”×’×“×œ×ª ×—×œ×•×Ÿ ×”×§×©×¨ ×œ-8K tokens
ollama run gemma3 --context-length 8192
```

### 4. ×©××™×¨×ª ×©×™×—×•×ª

```python
import ollama
import json
from datetime import datetime

def save_chat(messages, filename=None):
    if not filename:
        filename = f"chat_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    
    with open(filename, 'w', encoding='utf-8') as f:
        json.dump(messages, f, ensure_ascii=False, indent=2)
    
    print(f"ğŸ’¾ ×©×™×—×” × ×©××¨×” ×‘-{filename}")

# ×“×•×’××ª ×©×™××•×©
messages = [
    {'role': 'user', 'content': '×”×™×™'},
    {'role': 'assistant', 'content': '×©×œ×•×! ××™×š ××•×›×œ ×œ×¢×–×•×¨?'}
]

save_chat(messages)
```

### 5. ×©×¨×ª ××¨×•×—×§ ×¢× Ollama

#### ×”×¤×¢×œ×ª ×©×¨×ª × ×’×™×© ××¨×—×•×§

```bash
# ×”×¤×¢×œ×” ×¢×œ ×›×œ ×”×××©×§×™×
OLLAMA_HOST=0.0.0.0:11434 ollama serve
```

#### ×”×ª×—×‘×¨×•×ª ××œ×§×•×— ××¨×•×—×§

```python
import ollama

# ×”×ª×—×‘×¨×•×ª ×œ×©×¨×ª ××¨×•×—×§
client = ollama.Client(host='http://192.168.1.100:11434')

response = client.chat(
    model='gemma3',
    messages=[{'role': 'user', 'content': 'hello'}]
)

print(response['message']['content'])
```

### 6. ×‘×“×™×§×ª ×‘×™×¦×•×¢×™× (Benchmarking)

```bash
#!/bin/bash

echo "ğŸ§ª ×‘×“×™×§×ª ×‘×™×¦×•×¢×™×..."

# ×‘×“×™×§×ª ×–××Ÿ ×ª×©×•×‘×”
time ollama run gemma3 "×¡×¤×•×¨ ×¢×“ 10" --verbose

# ×‘×“×™×§×ª tokens per second
ollama run llama3.2 "×›×ª×•×‘ ×¤×¡×§×” ×©×œ 100 ××™×œ×™×" --verbose 2>&1 | grep "tokens"
```

### 7. ××™× ×˜×’×¨×¦×™×” ×¢× LangChain

```bash
pip install langchain langchain-community
```

```python
from langchain_community.llms import Ollama
from langchain.prompts import PromptTemplate
from langchain.chains import LLMChain

# ×™×¦×™×¨×ª ××•×“×œ
llm = Ollama(model="llama3.2")

# ×™×¦×™×¨×ª template
prompt = PromptTemplate(
    input_variables=["topic"],
    template="×›×ª×•×‘ ××××¨ ×§×¦×¨ ×¢×œ {topic} ×‘×¢×‘×¨×™×ª"
)

# ×™×¦×™×¨×ª chain
chain = LLMChain(llm=llm, prompt=prompt)

# ×”×¨×¦×”
result = chain.run(topic="×‘×™× ×” ××œ××›×•×ª×™×ª")
print(result)
```

### 8. Multi-Modal: ×˜×§×¡×˜ + ×ª××•× ×•×ª

```python
import ollama
import base64

def analyze_image(image_path, question):
    with open(image_path, 'rb') as f:
        image_data = base64.b64encode(f.read()).decode('utf-8')
    
    response = ollama.chat(
        model='llama3.2-vision',
        messages=[{
            'role': 'user',
            'content': question,
            'images': [image_data]
        }]
    )
    
    return response['message']['content']

# ×©×™××•×©
result = analyze_image('document.png', '×ª××¦×ª ××ª ×”××¡××š ×”×–×”')
print(result)
```

---

## ğŸ”§ ×¤×ª×¨×•×Ÿ ×‘×¢×™×•×ª × ×¤×•×¦×•×ª

### âŒ ×‘×¢×™×”: "connection refused" ××• "cannot connect"

**×¤×ª×¨×•×Ÿ:**

```bash
# 1. ×‘×“×™×§×” ×× ×”×©×™×¨×•×ª ×¨×¥
ps aux | grep ollama

# 2. ×”×¤×¢×œ×ª ×”×©×™×¨×•×ª ×™×“× ×™×ª
ollama serve

# 3. ×‘×“×™×§×ª ×¤×•×¨×˜
netstat -tuln | grep 11434

# 4. Linux - ×‘×“×™×§×ª systemd
sudo systemctl status ollama
sudo systemctl restart ollama
```

### âŒ ×‘×¢×™×”: "model not found"

**×¤×ª×¨×•×Ÿ:**

```bash
# ×¨×©×™××ª ××•×“×œ×™× ×–××™× ×™×
ollama list

# ×”×•×¨×“×ª ×”××•×“×œ ××—×“×©
ollama pull <model-name>

# × ×™×§×•×™ cache
rm -rf ~/.ollama/models/<model-name>
ollama pull <model-name>
```

### âŒ ×‘×¢×™×”: ×‘×™×¦×•×¢×™× ××™×˜×™×™× ×××•×“

**××‘×—×•×Ÿ:**

```bash
# ×‘×“×™×§×” ×× ××©×ª××©×™× ×‘-GPU
ollama run llama3.2 --verbose "test" 2>&1 | grep -i gpu

# Linux: ×‘×“×™×§×ª GPU
nvidia-smi
```

**×¤×ª×¨×•× ×•×ª:**

```bash
# 1. ×”×•×¨×“×ª ××•×“×œ ×§×˜×Ÿ ×™×•×ª×¨
ollama pull gemma3:1b

# 2. ×©×™××•×© ×‘×’×¨×¡×” ×“×—×•×¡×”
ollama pull llama3.2:Q4_K_M

# 3. ×”×§×˜× ×ª context window
ollama run llama3.2 --context-length 2048

# 4. ×”×’×“×œ×ª ××¡×¤×¨ threads
export OLLAMA_NUM_THREADS=8
```

### âŒ ×‘×¢×™×”: ×©×’×™××ª "out of memory"

**×¤×ª×¨×•× ×•×ª:**

```bash
# 1. ×¡×’×™×¨×ª ×ª×•×›× ×™×•×ª ××—×¨×•×ª

# 2. ×©×™××•×© ×‘××•×“×œ ×§×˜×Ÿ ×™×•×ª×¨
ollama pull gemma3:1b

---

## ğŸ“Š ×¡×˜×˜×™×¡×˜×™×§×•×ª ×”×¤×¨×•×™×§×˜

- **×›×•×›×‘×™×**: 156,999 â­
- **Forks**: 13,816 ğŸ”±
- **Issues**: 2,315 ğŸ›
- **×©×¤×”**: Go ğŸ’»
- **×¨×™×©×™×•×Ÿ**: MIT License ğŸ“œ

## ğŸ”— ×§×™×©×•×¨×™× ×©×™××•×©×™×™×

- [×¨×™×¤×• ×‘-GitHub](https://github.com/ollama/ollama)
- [Issues & ×ª××™×›×”](https://github.com/ollama/ollama/issues)
- [Discussions](https://github.com/ollama/ollama/discussions)
- [Wiki](https://github.com/ollama/ollama/wiki)

---

*××“×¨×™×š ×–×” × ×•×¦×¨ ××•×˜×•××˜×™×ª ×¢×œ ×™×“×™ AI Guide Bot ×¢× Claude AI*
*×¢×“×›×•×Ÿ ××—×¨×•×Ÿ: 03/12/2025 18:58*
